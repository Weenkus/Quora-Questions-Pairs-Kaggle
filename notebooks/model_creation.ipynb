{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_old = pd.read_pickle('../features/train_new.pkl')\n",
    "print(train_old.shape)\n",
    "\n",
    "train_bekavac = pd.read_csv('../features/train_features_bekavac.csv')\n",
    "print(train_bekavac.shape)\n",
    "\n",
    "train_magic1 = pd.read_csv('../features/train_magic_feature_v1.csv')\n",
    "print(train_magic1.shape)\n",
    "\n",
    "train_magic2 = pd.read_csv('../features/train_magic_feature_v2.csv')\n",
    "print(train_magic2.shape)\n",
    "\n",
    "train_magic3 = pd.read_csv('../features/train_magic_feature_v3.csv')\n",
    "print(train_magic3.shape)\n",
    "\n",
    "abhishek_train = pd.read_csv('../features/abhishek_train_features.csv', encoding=\"ISO-8859-1\")\n",
    "print(abhishek_train.shape)\n",
    "\n",
    "train_magic1 = train_magic1.drop('is_duplicate', 1)\n",
    "train_magic1 = train_magic1.drop('question2', 1)\n",
    "train_magic1 = train_magic1.drop('question1', 1)\n",
    "\n",
    "abhishek_train = abhishek_train.drop('question2', 1)\n",
    "abhishek_train = abhishek_train.drop('question1', 1)\n",
    "\n",
    "train = pd.concat([train_old, train_bekavac, train_magic1, train_magic2, train_magic3, abhishek_train], axis=1)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_old = pd.read_pickle('../features/test_new.pkl')\n",
    "print(test_old.shape)\n",
    "\n",
    "test_bekavac = pd.read_csv('../features/test_features_bekavac.csv')\n",
    "print(test_bekavac.shape)\n",
    "\n",
    "test_magic1 = pd.read_csv('../features/test_magic_feature_v1.csv')\n",
    "print(test_magic1.shape)\n",
    "\n",
    "test_magic1 = test_magic1.drop('is_duplicate', 1)\n",
    "test_magic1 = test_magic1.drop('question2', 1)\n",
    "test_magic1 = test_magic1.drop('question1', 1)\n",
    "\n",
    "test_magic2 = pd.read_csv('../features/test_magic_feature_v2.csv')\n",
    "print(test_magic2.shape)\n",
    "\n",
    "test_magic3 = pd.read_csv('../features/test_magic_feature_v3.csv')\n",
    "print(test_magic3.shape)\n",
    "\n",
    "abhishek_test = pd.read_csv('../features/abhishek_test_features.csv', encoding=\"ISO-8859-1\")\n",
    "print(abhishek_test.shape)\n",
    "\n",
    "abhishek_test = abhishek_test.drop('question2', 1)\n",
    "abhishek_test = abhishek_test.drop('question1', 1)\n",
    "\n",
    "test = pd.concat([test_old, test_bekavac, test_magic1, test_magic2, test_magic3, abhishek_test], axis=1)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# features = ['cosin_sim', 'word_share', 'q1_char_num', 'q1_word_num', 'q2_char_num', 'q2_word_num',\n",
    "#             'start_with_same_world', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "#            'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1', 'avg_word_len_q2',\n",
    "#            'avg_word_difference', 'unigrams_common_count', 'bigrams_common_count', 'unigrams_common_ratio',\n",
    "#            'bigrams_common_ratio', 'word2vec_q1_mean', 'word2vec_q2_mean']\n",
    "\n",
    "# features = ['q1_hash', 'q2_hash', 'q1_freq', 'q2_freq', 'q1_q2_intersect',\n",
    "#             'word_share',\n",
    "#        'start_with_same_world', 'q1_char_num', 'q2_char_num', 'q1_word_num',\n",
    "#        'q2_word_num', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "#        'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1',\n",
    "#        'avg_word_len_q2', 'avg_word_difference', 'unigrams_common_count',\n",
    "#        'bigrams_common_count', 'unigrams_common_ratio', 'bigrams_common_ratio',\n",
    "#        'cosin_sim', 'word2vec_q1_mean', 'word2vec_q2_mean', 'q1_NN_count',\n",
    "#        'q2_NN_count', 'NN_diff', 'q1_RB_count', 'q2_RB_count', 'RB_diff',\n",
    "#        'q1_VB_count', 'q2_VB_count', 'VB_diff', 'q1_DT_count', 'q2_DT_count',\n",
    "#        'DT_diff', 'q1_JJ_count', 'q2_JJ_count', 'JJ_diff', 'q1_FW_count',\n",
    "#        'q2_FW_count', 'FW_diff', 'q1_RP_count', 'q2_RP_count', 'RP_diff',\n",
    "#        'q1_SYM_count', 'q2_SYM_count', 'SYM_diff',\n",
    "#        'document_pos_similarity_10_feature',\n",
    "#        'document_pos_similarity_3_feature', 'entities_similarity_feature',\n",
    "#        'heads_similarity_feature', 'interrogative_match_feature',\n",
    "#        'non_alphanumeric_similarity_feature',\n",
    "#        'number_of_children_similarity_5_feature', 'numbers_similarity_feature',\n",
    "#        'objects_similarity_feature', 'roots_similarity_feature',\n",
    "#        'spacy_similarity_feature', 'subject_verb_inversion_similarity_feature',\n",
    "#        'subjects_similarity_feature',\n",
    "#        'unigram_idf_cutoff_similarity_10_feature',\n",
    "#        'unigram_idf_cutoff_similarity_15_feature',\n",
    "#        'unigram_idf_cutoff_similarity_5_feature',\n",
    "#        'unigram_idf_mean_difference_feature']\n",
    "\n",
    "features = ['q1_freq', 'q2_freq', 'q1_q2_intersect',\n",
    "            'word_share',\n",
    "       'start_with_same_world', 'q1_char_num', 'q2_char_num', 'q1_word_num',\n",
    "       'q2_word_num', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "       'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1',\n",
    "       'avg_word_len_q2', 'avg_word_difference', 'unigrams_common_count',\n",
    "       'bigrams_common_count', 'unigrams_common_ratio', 'bigrams_common_ratio',\n",
    "       'cosin_sim', 'word2vec_q1_mean', 'word2vec_q2_mean', 'q1_NN_count',\n",
    "       'q2_NN_count', 'NN_diff', 'q1_RB_count', 'q2_RB_count', 'RB_diff',\n",
    "       'q1_VB_count', 'q2_VB_count', 'VB_diff', 'q1_DT_count', 'q2_DT_count',\n",
    "       'DT_diff', 'q1_JJ_count', 'q2_JJ_count', 'JJ_diff', 'q1_FW_count',\n",
    "       'q2_FW_count', 'FW_diff', 'q1_RP_count', 'q2_RP_count', 'RP_diff',\n",
    "       'q1_SYM_count', 'q2_SYM_count', 'SYM_diff',\n",
    "       'document_pos_similarity_10_feature',\n",
    "       'document_pos_similarity_3_feature', 'entities_similarity_feature',\n",
    "       'heads_similarity_feature', 'interrogative_match_feature',\n",
    "       'non_alphanumeric_similarity_feature',\n",
    "       'number_of_children_similarity_5_feature', 'numbers_similarity_feature',\n",
    "       'objects_similarity_feature', 'roots_similarity_feature',\n",
    "       'spacy_similarity_feature', 'subject_verb_inversion_similarity_feature',\n",
    "       'subjects_similarity_feature',\n",
    "       'unigram_idf_cutoff_similarity_10_feature',\n",
    "       'unigram_idf_cutoff_similarity_15_feature',\n",
    "       'unigram_idf_cutoff_similarity_5_feature',\n",
    "       'unigram_idf_mean_difference_feature']\n",
    "\n",
    "target = 'is_duplicate'\n",
    "\n",
    "X = train[features]\n",
    "y = train[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pos_train = X[y == 1]\n",
    "neg_train = X[y == 0]\n",
    "\n",
    "# Now we oversample the negative class\n",
    "# There is likely a much more elegant way to do this...\n",
    "p = 0.174\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "X = pd.concat([pos_train, neg_train])\n",
    "y = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_vald, y_train, y_vald = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_vald_scaled = scaler.transform(X_vald)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "\n",
    "X_train = scaler.transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, log_loss\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_jobs=8)   # 0.39680 (on public)\n",
    "#rfc = RandomForestClassifier(max_depth=40, n_estimators=56, n_jobs=8)\n",
    "#model = ExtraTreesClassifier(n_estimators=200, n_jobs=8) # 0.48183 (on public)\n",
    "#model = AdaBoostClassifier()\n",
    "#gbc = GradientBoostingClassifier(n_estimators=500, max_depth=4, learning_rate=0.2, subsample=0.7) # 0.34721 (on public)\n",
    "#model = KNeighborsClassifier(n_jobs=8, n_neighbors=25)\n",
    "#model = BernoulliNB(alpha=0.01) # 0.57\n",
    "#model = SVC()\n",
    "#model = LogisticRegression(max_iter=500, C=2, tol=0.01)\n",
    "#model = LDA()\n",
    "#model = QDA(reg_param=0.9)\n",
    "#sgd = SGDClassifier(loss='log', penalty='elasticnet', n_jobs=8, n_iter=50, eta0=0.5, epsilon=0.5)\n",
    "\n",
    "#model.fit(X_train, y_train, test_data=[(X_test, y_test)])\n",
    "\n",
    "#model = ExtraTreesClassifier(n_estimators=58, n_jobs=8, max_depth=40)\n",
    "\n",
    "\n",
    "model = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth=5, subsample=0.65, gamma=1.5, seed=42,\n",
    "            colsample_bytree=0.7) # 0.34785 (on public)\n",
    "\n",
    "# xgb2 = XGBClassifier(n_estimators=700, learning_rate=0.1, max_depth=6, subsample=0.65, gamma=1.5, seed=42, \n",
    "#                      colsample_bytree=0.7)\n",
    "\n",
    "# xgb3 = XGBClassifier(n_estimators=500, learning_rate=0.2, max_depth=4, subsample=0.7, gamma=0.5, seed=42, \n",
    "#                      colsample_bytree=0.7)\n",
    "\n",
    "# xgb4 = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth=5, subsample=0.65, gamma=1.5, seed=1234,\n",
    "#             colsample_bytree=0.7) # 0.34785 (on public)\n",
    "\n",
    "# xgb5 = XGBClassifier(n_estimators=700, learning_rate=0.1, max_depth=6, subsample=0.65, gamma=1.5, seed=5552, \n",
    "#                      colsample_bytree=0.7)\n",
    "\n",
    "# xgb6 = XGBClassifier(n_estimators=500, learning_rate=0.2, max_depth=4, subsample=0.7, gamma=0.5, seed=7121, \n",
    "#                      colsample_bytree=0.7)\n",
    "\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_vald, y_vald)],\n",
    "         early_stopping_rounds=50, verbose=True, eval_metric='logloss')\n",
    "\n",
    "\n",
    "# model = VotingClassifier(estimators=[('xgb1', xgb1), ('xgb2', xgb2), ('xgb3', xgb3), ('xgb4', xgb4),\n",
    "#                                      ('xgb5', xgb5), ('xgb6', xgb6)],\n",
    "#                          voting='soft', weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_jobs=8) \n",
    "parameters = {\n",
    "    'n_estimators' : [10, 25, 50, 80],\n",
    "    'max_depth' : [3, 4, 6, 12, 20, 30, 40],\n",
    "    'min_samples_leaf' : [1, 2, 3],\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'n_jobs': [8]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv=5, scoring=\"log_loss\", verbose=5, n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "best_estimator = clf.best_estimator_\n",
    "print('Best hyperparameters: ' + str(clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "parameters = {\n",
    "    'n_estimators' : [50],\n",
    "    'max_depth' : [4, 6, 12, 35],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1, 0.2],\n",
    "    'gamma': [0.5, 1.5, 3, 5],\n",
    "    'subsample': [1, 0.7],\n",
    "    'colsample_bytree': [0.7, 1]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv=5, scoring=\"log_loss\", verbose=5, n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "best_estimator = clf.best_estimator_\n",
    "print('Best hyperparameters: ' + str(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val_predictions = model.predict(X_vald_scaled)\n",
    "val_prob_predictions = model.predict_proba(X_vald_scaled)\n",
    "\n",
    "for metric_name, metric_func in zip(\n",
    "    ['F1-score', 'Acc', 'Precision', 'Recall', 'LogLoss'],\n",
    "    [f1_score, accuracy_score, precision_score, recall_score, log_loss]\n",
    "):\n",
    "    \n",
    "    val_predictions = val_predictions if metric_name not in ['LogLoss'] else val_prob_predictions\n",
    "    metric_score = metric_func(y_vald, val_predictions)\n",
    "    print('{0}: {1}'.format(metric_name, metric_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=500, learning_rate=0.02, max_depth=7, subsample=0.6, gamma=1.5, seed=42,\n",
    "            colsample_bytree=0.7)\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = 0.174 / 0.37\n",
    "b = (1 - 0.174) / (1 - 0.37)\n",
    "\n",
    "def fix_predictions_for_test_distribution(x):\n",
    "    return a * x / (a * x + b * (1 - x))\n",
    "\n",
    "predictions = np.array(list(map(fix_predictions_for_test_distribution, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    '../submissions/submission_xgb_1_', np.c_[range(len(predictions)), predictions[:,1]],\n",
    "    delimiter=',', header='test_id,is_duplicate', comments='', fmt='%d,%f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
