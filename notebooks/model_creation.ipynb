{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../features/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_pickle('../features/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['cosin_sim', 'word_share', 'q1_char_num', 'q1_word_num', 'q2_char_num', 'q2_word_num',\n",
    "            'start_with_same_world', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "           'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1', 'avg_word_len_q2',\n",
    "           'avg_word_difference', 'unigrams_common_count', 'bigrams_common_count', 'unigrams_common_ratio',\n",
    "           'bigrams_common_ratio', 'word2vec_q1_mean', 'word2vec_q2_mean']\n",
    "\n",
    "# Features gotten using RF for feature importance (only features with score 0.03 or greater were taken)\n",
    "# features = ['cosin_sim', 'word_share', 'q1_char_num', 'q2_char_num', 'rfidf_share', 'char_difference', \n",
    "#             'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1', 'avg_word_len_q2', \n",
    "#             'avg_word_difference', 'bigrams_common_count', 'unigrams_common_ratio', 'bigrams_common_ratio', \n",
    "#             'word2vec_q1_mean', 'word2vec_q2_mean']\n",
    "\n",
    "# RF 0.05 or greater feature importance\n",
    "#features = ['cosin_sim', 'word_share', 'rfidf_share',\n",
    "#            'bigrams_common_ratio', 'word2vec_q1_mean', 'word2vec_q2_mean']\n",
    "\n",
    "target = 'is_duplicate'\n",
    "\n",
    "X, y = train[features], train[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19124366100096607\n"
     ]
    }
   ],
   "source": [
    "pos_train = X[y == 1]\n",
    "neg_train = X[y == 0]\n",
    "\n",
    "# Now we oversample the negative class\n",
    "# There is likely a much more elegant way to do this...\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "X = pd.concat([pos_train, neg_train])\n",
    "y = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_vald, y_train, y_vald = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model works fine without scaling\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_vald_scaled = scaler.transform(X_vald)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, log_loss\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_jobs=8)   # 0.39680 (on public)\n",
    "#rfc = RandomForestClassifier(max_depth=40, n_estimators=56, n_jobs=8)\n",
    "#model = ExtraTreesClassifier(n_estimators=200, n_jobs=8) # 0.48183 (on public)\n",
    "#model = AdaBoostClassifier()\n",
    "#gbc = GradientBoostingClassifier(n_estimators=500, max_depth=4, learning_rate=0.2, subsample=0.7) # 0.34721 (on public)\n",
    "#model = KNeighborsClassifier(n_jobs=8, n_neighbors=25)\n",
    "#model = BernoulliNB(alpha=0.01) # 0.57\n",
    "#model = SVC()\n",
    "#model = LogisticRegression(max_iter=500, C=2, tol=0.01)\n",
    "#model = LDA()\n",
    "#model = QDA(reg_param=0.9)\n",
    "#sgd = SGDClassifier(loss='log', penalty='elasticnet', n_jobs=8, n_iter=50, eta0=0.5, epsilon=0.5)\n",
    "\n",
    "#model.fit(X_train, y_train, test_data=[(X_test, y_test)])\n",
    "\n",
    "#model = ExtraTreesClassifier(n_estimators=58, n_jobs=8, max_depth=40)\n",
    "\n",
    "\n",
    "xgb1 = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth=5, subsample=0.65, gamma=1.5, seed=42,\n",
    "            colsample_bytree=0.7) # 0.34785 (on public)\n",
    "\n",
    "xgb2 = XGBClassifier(n_estimators=700, learning_rate=0.1, max_depth=6, subsample=0.65, gamma=1.5, seed=42, \n",
    "                     colsample_bytree=0.7)\n",
    "\n",
    "xgb3 = XGBClassifier(n_estimators=500, learning_rate=0.2, max_depth=4, subsample=0.7, gamma=0.5, seed=42, \n",
    "                     colsample_bytree=0.7)\n",
    "\n",
    "xgb4 = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth=5, subsample=0.65, gamma=1.5, seed=1234,\n",
    "            colsample_bytree=0.7) # 0.34785 (on public)\n",
    "\n",
    "xgb5 = XGBClassifier(n_estimators=700, learning_rate=0.1, max_depth=6, subsample=0.65, gamma=1.5, seed=5552, \n",
    "                     colsample_bytree=0.7)\n",
    "\n",
    "xgb6 = XGBClassifier(n_estimators=500, learning_rate=0.2, max_depth=4, subsample=0.7, gamma=0.5, seed=7121, \n",
    "                     colsample_bytree=0.7)\n",
    "\n",
    "#model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_vald, y_vald)],\n",
    "#         early_stopping_rounds=50, verbose=True, eval_metric='logloss')\n",
    "\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb1', xgb1), ('xgb2', xgb2), ('xgb3', xgb3), ('xgb4', xgb4),\n",
    "                                     ('xgb5', xgb5), ('xgb6', xgb6)],\n",
    "                         voting='soft', weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_jobs=8) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_features = []\n",
    "for feature, importance in zip(features, model.feature_importances_):\n",
    "    if importance > 0.05:\n",
    "        new_features.append(feature)\n",
    "        \n",
    "print(new_features)\n",
    "\n",
    "X = X[new_features]\n",
    "X_test = X_test[new_features]\n",
    "\n",
    "# ['cosin_sim', 'word_share', 'q1_char_num', 'q2_char_num', 'rfidf_share', 'char_difference',\n",
    "# 'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1', 'avg_word_len_q2', \n",
    "# 'avg_word_difference', 'bigrams_common_count', 'unigrams_common_ratio', 'bigrams_common_ratio',\n",
    "# 'word2vec_q1_mean', 'word2vec_q2_mean'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_jobs=8) \n",
    "parameters = {\n",
    "    'n_estimators' : [10, 25, 50, 80],\n",
    "    'max_depth' : [3, 4, 6, 12, 20, 30, 40],\n",
    "    'min_samples_leaf' : [1, 2, 3],\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'n_jobs': [8]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv=5, scoring=\"log_loss\", verbose=5, n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "best_estimator = clf.best_estimator_\n",
    "print('Best hyperparameters: ' + str(clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "parameters = {\n",
    "    'n_estimators' : [50],\n",
    "    'max_depth' : [4, 6, 12, 35],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1, 0.2],\n",
    "    'gamma': [0.5, 1.5, 3, 5],\n",
    "    'subsample': [1, 0.7],\n",
    "    'colsample_bytree': [0.7, 1]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv=5, scoring=\"log_loss\", verbose=5, n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "best_estimator = clf.best_estimator_\n",
    "print('Best hyperparameters: ' + str(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_predictions = model.predict(X_vald_scaled)\n",
    "val_prob_predictions = model.predict_proba(X_vald_scaled)\n",
    "\n",
    "for metric_name, metric_func in zip(\n",
    "    ['F1-score', 'Acc', 'Precision', 'Recall', 'LogLoss'],\n",
    "    [f1_score, accuracy_score, precision_score, recall_score, log_loss]\n",
    "):\n",
    "    \n",
    "    val_predictions = val_predictions if metric_name not in ['LogLoss'] else val_prob_predictions\n",
    "    metric_score = metric_func(y_vald, val_predictions)\n",
    "    print('{0}: {1}'.format(metric_name, metric_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "predictions = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    '../submissions/submission.csv', np.c_[range(len(predictions)), predictions[:,1]],\n",
    "    delimiter=',', header='test_id,is_duplicate', comments='', fmt='%d,%f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
