{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data with features (pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../features/train.pkl')\n",
    "test = pd.read_pickle('../features/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> all\n",
      "    Downloading collection 'all'\n",
      "       | \n",
      "       | Downloading package abc to /root/nltk_data...\n",
      "       |   Package abc is already up-to-date!\n",
      "       | Downloading package alpino to /root/nltk_data...\n",
      "       |   Package alpino is already up-to-date!\n",
      "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
      "       |   Package biocreative_ppi is already up-to-date!\n",
      "       | Downloading package brown to /root/nltk_data...\n",
      "       |   Package brown is already up-to-date!\n",
      "       | Downloading package brown_tei to /root/nltk_data...\n",
      "       |   Package brown_tei is already up-to-date!\n",
      "       | Downloading package cess_cat to /root/nltk_data...\n",
      "       |   Package cess_cat is already up-to-date!\n",
      "       | Downloading package cess_esp to /root/nltk_data...\n",
      "       |   Package cess_esp is already up-to-date!\n",
      "       | Downloading package chat80 to /root/nltk_data...\n",
      "       |   Package chat80 is already up-to-date!\n",
      "       | Downloading package city_database to /root/nltk_data...\n",
      "       |   Package city_database is already up-to-date!\n",
      "       | Downloading package cmudict to /root/nltk_data...\n",
      "       |   Package cmudict is already up-to-date!\n",
      "       | Downloading package comparative_sentences to\n",
      "       |     /root/nltk_data...\n",
      "       |   Package comparative_sentences is already up-to-date!\n",
      "       | Downloading package comtrans to /root/nltk_data...\n",
      "       |   Package comtrans is already up-to-date!\n",
      "       | Downloading package conll2000 to /root/nltk_data...\n",
      "       |   Package conll2000 is already up-to-date!\n",
      "       | Downloading package conll2002 to /root/nltk_data...\n",
      "       |   Package conll2002 is already up-to-date!\n",
      "       | Downloading package conll2007 to /root/nltk_data...\n",
      "       |   Package conll2007 is already up-to-date!\n",
      "       | Downloading package crubadan to /root/nltk_data...\n",
      "       |   Package crubadan is already up-to-date!\n",
      "       | Downloading package dependency_treebank to /root/nltk_data...\n",
      "       |   Package dependency_treebank is already up-to-date!\n",
      "       | Downloading package europarl_raw to /root/nltk_data...\n",
      "       |   Package europarl_raw is already up-to-date!\n",
      "       | Downloading package floresta to /root/nltk_data...\n",
      "       |   Package floresta is already up-to-date!\n",
      "       | Downloading package framenet_v15 to /root/nltk_data...\n",
      "       |   Package framenet_v15 is already up-to-date!\n",
      "       | Downloading package framenet_v17 to /root/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v17.zip.\n",
      "       | Downloading package gazetteers to /root/nltk_data...\n",
      "       |   Unzipping corpora/gazetteers.zip.\n",
      "       | Downloading package genesis to /root/nltk_data...\n",
      "       |   Unzipping corpora/genesis.zip.\n",
      "       | Downloading package gutenberg to /root/nltk_data...\n",
      "       |   Unzipping corpora/gutenberg.zip.\n",
      "       | Downloading package ieer to /root/nltk_data...\n",
      "       |   Unzipping corpora/ieer.zip.\n",
      "       | Downloading package inaugural to /root/nltk_data...\n",
      "       |   Unzipping corpora/inaugural.zip.\n",
      "       | Downloading package indian to /root/nltk_data...\n",
      "       |   Unzipping corpora/indian.zip.\n",
      "       | Downloading package jeita to /root/nltk_data...\n",
      "       | Downloading package kimmo to /root/nltk_data...\n",
      "       |   Unzipping corpora/kimmo.zip.\n",
      "       | Downloading package knbc to /root/nltk_data...\n",
      "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
      "       |   Unzipping corpora/lin_thesaurus.zip.\n",
      "       | Downloading package mac_morpho to /root/nltk_data...\n",
      "       |   Unzipping corpora/mac_morpho.zip.\n",
      "       | Downloading package machado to /root/nltk_data...\n",
      "       | Downloading package masc_tagged to /root/nltk_data...\n",
      "       | Downloading package moses_sample to /root/nltk_data...\n",
      "       |   Unzipping models/moses_sample.zip.\n",
      "       | Downloading package movie_reviews to /root/nltk_data...\n",
      "       |   Unzipping corpora/movie_reviews.zip.\n",
      "       | Downloading package names to /root/nltk_data...\n",
      "       |   Unzipping corpora/names.zip.\n",
      "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "       | Downloading package nps_chat to /root/nltk_data...\n",
      "       |   Unzipping corpora/nps_chat.zip.\n",
      "       | Downloading package omw to /root/nltk_data...\n",
      "       |   Unzipping corpora/omw.zip.\n",
      "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
      "       |   Unzipping corpora/opinion_lexicon.zip.\n",
      "       | Downloading package paradigms to /root/nltk_data...\n",
      "       |   Unzipping corpora/paradigms.zip.\n",
      "       | Downloading package pil to /root/nltk_data...\n",
      "       |   Unzipping corpora/pil.zip.\n",
      "       | Downloading package pl196x to /root/nltk_data...\n",
      "       |   Unzipping corpora/pl196x.zip.\n",
      "       | Downloading package ppattach to /root/nltk_data...\n",
      "       |   Unzipping corpora/ppattach.zip.\n",
      "       | Downloading package problem_reports to /root/nltk_data...\n",
      "       |   Unzipping corpora/problem_reports.zip.\n",
      "       | Downloading package propbank to /root/nltk_data...\n",
      "       | Downloading package ptb to /root/nltk_data...\n",
      "       |   Unzipping corpora/ptb.zip.\n",
      "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_1.zip.\n",
      "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_2.zip.\n",
      "       | Downloading package pros_cons to /root/nltk_data...\n",
      "       |   Unzipping corpora/pros_cons.zip.\n",
      "       | Downloading package qc to /root/nltk_data...\n",
      "       |   Unzipping corpora/qc.zip.\n",
      "       | Downloading package reuters to /root/nltk_data...\n",
      "       | Downloading package rte to /root/nltk_data...\n",
      "       |   Unzipping corpora/rte.zip.\n",
      "       | Downloading package semcor to /root/nltk_data...\n",
      "       | Downloading package senseval to /root/nltk_data...\n",
      "       |   Unzipping corpora/senseval.zip.\n",
      "       | Downloading package sentiwordnet to /root/nltk_data...\n",
      "       |   Unzipping corpora/sentiwordnet.zip.\n",
      "       | Downloading package sentence_polarity to /root/nltk_data...\n",
      "       |   Unzipping corpora/sentence_polarity.zip.\n",
      "       | Downloading package shakespeare to /root/nltk_data...\n",
      "       |   Unzipping corpora/shakespeare.zip.\n",
      "       | Downloading package sinica_treebank to /root/nltk_data...\n",
      "       |   Unzipping corpora/sinica_treebank.zip.\n",
      "       | Downloading package smultron to /root/nltk_data...\n",
      "       |   Unzipping corpora/smultron.zip.\n",
      "       | Downloading package state_union to /root/nltk_data...\n",
      "       |   Unzipping corpora/state_union.zip.\n",
      "       | Downloading package stopwords to /root/nltk_data...\n",
      "       |   Unzipping corpora/stopwords.zip.\n",
      "       | Downloading package subjectivity to /root/nltk_data...\n",
      "       |   Unzipping corpora/subjectivity.zip.\n",
      "       | Downloading package swadesh to /root/nltk_data...\n",
      "       |   Unzipping corpora/swadesh.zip.\n",
      "       | Downloading package switchboard to /root/nltk_data...\n",
      "       |   Unzipping corpora/switchboard.zip.\n",
      "       | Downloading package timit to /root/nltk_data...\n",
      "       |   Unzipping corpora/timit.zip.\n",
      "       | Downloading package toolbox to /root/nltk_data...\n",
      "       |   Unzipping corpora/toolbox.zip.\n",
      "       | Downloading package treebank to /root/nltk_data...\n",
      "       |   Unzipping corpora/treebank.zip.\n",
      "       | Downloading package twitter_samples to /root/nltk_data...\n",
      "       |   Unzipping corpora/twitter_samples.zip.\n",
      "       | Downloading package udhr to /root/nltk_data...\n",
      "       |   Unzipping corpora/udhr.zip.\n",
      "       | Downloading package udhr2 to /root/nltk_data...\n",
      "       |   Unzipping corpora/udhr2.zip.\n",
      "       | Downloading package unicode_samples to /root/nltk_data...\n",
      "       |   Unzipping corpora/unicode_samples.zip.\n",
      "       | Downloading package universal_treebanks_v20 to\n",
      "       |     /root/nltk_data...\n",
      "       | Downloading package verbnet to /root/nltk_data...\n",
      "       |   Unzipping corpora/verbnet.zip.\n",
      "       | Downloading package webtext to /root/nltk_data...\n",
      "       |   Unzipping corpora/webtext.zip.\n",
      "       | Downloading package wordnet to /root/nltk_data...\n",
      "       |   Unzipping corpora/wordnet.zip.\n",
      "       | Downloading package wordnet_ic to /root/nltk_data...\n",
      "       |   Unzipping corpora/wordnet_ic.zip.\n",
      "       | Downloading package words to /root/nltk_data...\n",
      "       |   Unzipping corpora/words.zip.\n",
      "       | Downloading package ycoe to /root/nltk_data...\n",
      "       |   Unzipping corpora/ycoe.zip.\n",
      "       | Downloading package rslp to /root/nltk_data...\n",
      "       |   Unzipping stemmers/rslp.zip.\n",
      "       | Downloading package hmm_treebank_pos_tagger to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping taggers/hmm_treebank_pos_tagger.zip.\n",
      "       | Downloading package maxent_treebank_pos_tagger to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "       | Downloading package universal_tagset to /root/nltk_data...\n",
      "       |   Unzipping taggers/universal_tagset.zip.\n",
      "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
      "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "       | Downloading package punkt to /root/nltk_data...\n",
      "       |   Unzipping tokenizers/punkt.zip.\n",
      "       | Downloading package book_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/book_grammars.zip.\n",
      "       | Downloading package sample_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/sample_grammars.zip.\n",
      "       | Downloading package spanish_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/spanish_grammars.zip.\n",
      "       | Downloading package basque_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/basque_grammars.zip.\n",
      "       | Downloading package large_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/large_grammars.zip.\n",
      "       | Downloading package tagsets to /root/nltk_data...\n",
      "       |   Unzipping help/tagsets.zip.\n",
      "       | Downloading package snowball_data to /root/nltk_data...\n",
      "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
      "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "       | Downloading package word2vec_sample to /root/nltk_data...\n",
      "       |   Unzipping models/word2vec_sample.zip.\n",
      "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
      "       | Downloading package mte_teip5 to /root/nltk_data...\n",
      "       |   Unzipping corpora/mte_teip5.zip.\n",
      "       | Downloading package averaged_perceptron_tagger to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "       | Downloading package panlex_lite to /root/nltk_data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-18776207d219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;31m# function should make a new copy of self to use?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m                 \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1012\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'u'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_interactive_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_simple_interactive_download\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'    '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[1;32m    662\u001b[0m                                     subsequent_indent=prefix+prefix2+' '*4))\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m                 \u001b[0;31m# Error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mStartCollectionMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mFinishCollectionMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# If they gave us a list of ids, then download each one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_download_list\u001b[0;34m(self, items, download_dir, force)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProgressMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mProgressMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Handle Packages (delegate to a helper function).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_download_package\u001b[0;34m(self, info, download_dir, force)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 \u001b[0mnum_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 16k blocks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m                     \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m    927\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                   self.__class__)\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \"\"\"\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    for word in list(sentence):\n",
    "        if word in stops_words:\n",
    "            sentence.remove(word)  \n",
    "            \n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_dataframe(data):\n",
    "    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n",
    "    data = data.dropna(how=\"any\")\n",
    "    \n",
    "    for col in ['question1', 'question2']:\n",
    "        data[col] = data[col].apply(clean_sentence)\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_clean = clean_dataframe(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import gc\n",
    "\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "from simhash import Simhash\n",
    "\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus.reader.wordnet import ADJ, ADJ_SAT, ADV, NOUN, VERB\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    return data.apply(apply_func, axis=1, raw=True)\n",
    "\n",
    "def chunk(data, num):   \n",
    "    chunk_size = math.ceil(len(data) / num)\n",
    "    return [data[i*chunk_size : (i+1)*chunk_size] for i in range(num)]\n",
    "\n",
    "def pool_apply(data, proc_num=8):\n",
    "    \n",
    "    with Pool(processes=proc_num) as pool:\n",
    "        chunks = chunk(data, proc_num) \n",
    "        proccessed_chunks = list(pool.map(transform_data, chunks))\n",
    "  \n",
    "    return np.hstack(tuple(proccessed_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word_match_share(row):\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    \n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q1words[word] = 1\n",
    "            \n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q2words[word] = 1\n",
    "            \n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share(row):\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    \n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q1words[word] = 1\n",
    "            \n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q2words[word] = 1\n",
    "            \n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    with np.errstate(invalid='ignore'):\n",
    "        shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [\n",
    "            weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "        \n",
    "        total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "\n",
    "        R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "\n",
    "    return R if not math.isnan(R) else 0\n",
    "\n",
    "\n",
    "def start_with_same_first_word(row):\n",
    "    if not isinstance(row['question1'], str) or not isinstance(row['question2'], str):\n",
    "        return 0\n",
    "    \n",
    "    first_word_q1 = row['question1'].split()[0].lower()\n",
    "    first_word_q2 = row['question2'].split()[0].lower()\n",
    "    \n",
    "    return 1 if first_word_q1 == first_word_q2 else 0\n",
    "\n",
    "def question_length(row):\n",
    "    question = row[feature]\n",
    "    return len(question) if isinstance(question, str) else 0\n",
    "\n",
    "def word_count(row):\n",
    "    question = row[feature]\n",
    "    return len(question.split()) if isinstance(question, str) else 0\n",
    "\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "    \n",
    "def simhash_distance_seq(row):\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    if not isinstance(q1, str) or not isinstance(q2, str):\n",
    "        return 0\n",
    "\n",
    "    return Simhash(q1).distance(Simhash(q2))\n",
    "\n",
    "def simhash_distance_shingle(row):\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    if not isinstance(q1, str) or not isinstance(q2, str):\n",
    "        return 0\n",
    "    \n",
    "    q1_shingles = get_singles(q1)\n",
    "    q2_shingles = get_singles(q2)\n",
    "    \n",
    "    return Simhash(q1_shingles).distance(Simhash(q2_shingles))\n",
    "\n",
    "def get_singles(sequence, width = 3):\n",
    "    sequence = sequence.lower()\n",
    "    sequence = re.sub(r'[^\\w]+', '', sequence)\n",
    "    return [sequence[i:i + width] for i in range(max(len(sequence) - width + 1, 1))]\n",
    "\n",
    "\n",
    "\n",
    "def get_common_unigrams(row):\n",
    "    question1 = str(row['question1'])\n",
    "    question2 = str(row['question2'])\n",
    "    \n",
    "    q1_unigrams = set([i for i in nltk.ngrams(question1, 1)])\n",
    "    q2_unigrams = set([i for i in nltk.ngrams(question2, 1)])\n",
    "    return len( q1_unigrams.intersection(q2_unigrams))\n",
    "\n",
    "def get_common_unigram_ratio(row):\n",
    "    question1 = str(row['question1'])\n",
    "    question2 = str(row['question2'])\n",
    "    \n",
    "    q1_unigrams = set([i for i in nltk.ngrams(question1, 1)])\n",
    "    q2_unigrams = set([i for i in nltk.ngrams(question2, 1)])\n",
    "    unigram_count = float(row[\"unigrams_common_count\"])\n",
    "               \n",
    "    return  unigram_count / max(len(q1_unigrams.union(q2_unigrams)),1)\n",
    "\n",
    "def get_common_bigrams(row):\n",
    "    question1 = str(row['question1'])\n",
    "    question2 = str(row['question2'])\n",
    "    \n",
    "    q1_bigrams = set([i for i in nltk.ngrams(question1, 2)])\n",
    "    q2_bigrams = set([i for i in nltk.ngrams(question2, 2)])\n",
    "    return len(q1_bigrams.intersection(q2_bigrams))\n",
    "\n",
    "def get_common_bigram_ratio(row):\n",
    "    question1 = str(row['question1'])\n",
    "    question2 = str(row['question2'])\n",
    "    \n",
    "    q1_bigrams = set([i for i in nltk.ngrams(question1, 2)])\n",
    "    q2_bigrams = set([i for i in nltk.ngrams(question2, 2)])\n",
    "    bigram_count = float(row[\"bigrams_common_count\"])\n",
    "               \n",
    "    return  bigram_count / max(len(q1_bigrams.union(q2_bigrams)),1)\n",
    "\n",
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for col in ['question1', 'question2']:\n",
    "        for sentence in data[col].iteritems():\n",
    "            word_list = sentence[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "            \n",
    "    return corpus\n",
    "\n",
    "def question_to_word2vec(question_string, word2vec_model):\n",
    "    \"\"\"\n",
    "    Given question string, returns word2vec vector of the questions tring\n",
    "    :param question_string : The given question as a string.\n",
    "    \"\"\"\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    if not isinstance(question_string, str):\n",
    "        return 0\n",
    "    \n",
    "    words = word_tokenize(question_string)[:-1]\n",
    "    non_stop_words = []\n",
    "    for word in words:\n",
    "        if word.lower().strip('-') not in stops_words:\n",
    "            word = WordNetLemmatizer().lemmatize(word, NOUN)\n",
    "            \n",
    "            if word.lower() in word2vec_model.wv:\n",
    "                non_stop_words.append(word.lower().strip('-'))\n",
    "            \n",
    "    if len(non_stop_words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    vectors = [word2vec_model.wv[word] for word in non_stop_words]\n",
    "    vector = sum(vectors)/float(len(non_stop_words))\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def numpy_cosine(row):\n",
    "    \"\"\"\n",
    "    Cosine similarity between q1 and q2 question instances using their vectors\n",
    "    :return: similarity between q1 and q2\n",
    "    \"\"\"\n",
    "    q1, q2 = row['question1'], row['question2']\n",
    "    q1_vec, q2_vec = question_to_word2vec(q1, word2vec_model), question_to_word2vec(q2, word2vec_model)\n",
    "    \n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cosine_similarity = np.dot(q1_vec, q2_vec) / (np.linalg.norm(q1_vec) * np.linalg.norm(q2_vec))\n",
    "    \n",
    "    return cosine_similarity if isinstance(cosine_similarity, np.float32) else 0.0\n",
    "\n",
    "def mean_word2vec(row):\n",
    "    question = row[feature]\n",
    "    return np.mean(question_to_word2vec(question, word2vec_model))\n",
    "\n",
    "# Run clean data in the first part of the notebook to generate the train_clean dataset\n",
    "corpus = build_corpus(train_clean)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.14919114e+00,   9.46997926e-02,   1.58904457e+00,\n",
       "        -1.99016500e+00,   8.28770339e-01,   9.26753059e-02,\n",
       "         9.13018882e-01,   3.21756482e+00,  -2.37470722e+00,\n",
       "        -9.34127271e-01,   1.68880677e+00,   1.47907805e+00,\n",
       "        -1.09974277e+00,   2.01751709e+00,   1.85012305e+00,\n",
       "         7.62449920e-01,  -8.39057207e-01,   5.67743368e-02,\n",
       "         2.13133955e+00,   1.84380814e-01,   2.27326274e+00,\n",
       "        -1.08375239e+00,   5.49883425e-01,   3.70966578e+00,\n",
       "         1.34487402e+00,  -1.43110168e+00,  -2.11985683e+00,\n",
       "        -1.01818264e-01,   2.73517817e-01,   4.50783491e-01,\n",
       "         1.75322378e+00,  -2.15832043e+00,   8.23113620e-01,\n",
       "         1.79154670e+00,  -7.90773869e-01,   6.57281399e-01,\n",
       "        -3.09041595e+00,  -1.45009851e+00,   6.23656213e-02,\n",
       "        -2.82265306e+00,  -1.20934260e+00,   1.25538063e+00,\n",
       "         1.82773590e+00,   8.68064344e-01,   3.06473851e-01,\n",
       "        -1.14920175e+00,   2.79344052e-01,   1.39572263e+00,\n",
       "         2.02216458e+00,  -1.64092183e+00,  -4.99980628e-01,\n",
       "         5.70000410e-02,  -1.32625413e+00,  -1.37902093e+00,\n",
       "        -6.26268864e-01,  -3.47844273e-01,  -5.42262495e-01,\n",
       "        -5.42536259e-01,  -1.53179657e+00,  -6.93481922e-01,\n",
       "         2.61538458e+00,   4.29830730e-01,   4.66299951e-01,\n",
       "         3.16203833e+00,   3.81326056e+00,  -2.09381032e+00,\n",
       "         3.11001450e-01,  -6.11341298e-01,   1.71988308e+00,\n",
       "        -1.06490612e+00,   1.83945668e+00,   1.14902985e+00,\n",
       "        -1.39170563e+00,  -8.58151242e-02,  -6.80629849e-01,\n",
       "        -6.93744957e-01,  -2.78704214e+00,   5.62625453e-02,\n",
       "         1.76618814e+00,   9.59653407e-04,   1.95398688e+00,\n",
       "         6.34724617e-01,   1.70900953e+00,   2.91958308e+00,\n",
       "         1.40624595e+00,   5.44173717e-01,   1.52142906e+00,\n",
       "        -3.01011658e+00,   3.05004144e+00,   1.58554101e+00,\n",
       "         1.56685686e+00,   1.94263363e+00,  -3.14539105e-01,\n",
       "        -1.08753633e+00,   2.99796700e-01,   7.00860560e-01,\n",
       "         1.21974707e-01,  -4.91547972e-01,  -4.25570726e-01,\n",
       "        -2.45236373e+00], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=500, workers=8)\n",
    "word2vec_model.wv['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eps = 5000 \n",
    "train_qs = pd.Series(train['question1'].tolist() + train['question2'].tolist()).astype(str)\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What would a Trump presidency mean for current...</td>\n",
       "      <td>How will a Trump presidency affect the student...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What does manipulation mean?</td>\n",
       "      <td>What does manipulation means?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Why are so many Quora users posting questions ...</td>\n",
       "      <td>Why do people ask Quora questions which can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Why do rockets look white?</td>\n",
       "      <td>Why are rockets and boosters painted white?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>How should I prepare for CA final law?</td>\n",
       "      <td>How one should know that he/she completely pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What are some special cares for someone with a...</td>\n",
       "      <td>How can I keep my nose from getting stuffy at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>What Game of Thrones villain would be the most...</td>\n",
       "      <td>What Game of Thrones villain would you most li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>How do we prepare for UPSC?</td>\n",
       "      <td>How do I prepare for civil service?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>What are some examples of products that can be...</td>\n",
       "      <td>What are some of the products made from crude ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>How do I make friends.</td>\n",
       "      <td>How to make friends ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Is Career Launcher good for RBI Grade B prepar...</td>\n",
       "      <td>How is career launcher online program for RBI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Will a Blu Ray play on a regular DVD player? I...</td>\n",
       "      <td>How can you play a Blu Ray DVD on a regular DV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>What is the best/most memorable thing you've e...</td>\n",
       "      <td>What is the most delicious dish you've ever ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>I was suddenly logged off Gmail. I can't remem...</td>\n",
       "      <td>I can't remember my Gmail password or my recov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>How is the new Harry Potter book 'Harry Potter...</td>\n",
       "      <td>How bad is the new book by J.K Rowling?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question1  \\\n",
       "5   Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "7                      How can I be a good geologist?   \n",
       "11        How do I read and find my YouTube comments?   \n",
       "12               What can make Physics easy to learn?   \n",
       "13        What was your first sexual experience like?   \n",
       "15  What would a Trump presidency mean for current...   \n",
       "16                       What does manipulation mean?   \n",
       "18  Why are so many Quora users posting questions ...   \n",
       "20                         Why do rockets look white?   \n",
       "29             How should I prepare for CA final law?   \n",
       "31  What are some special cares for someone with a...   \n",
       "32  What Game of Thrones villain would be the most...   \n",
       "38                        How do we prepare for UPSC?   \n",
       "48  What are some examples of products that can be...   \n",
       "49                             How do I make friends.   \n",
       "50  Is Career Launcher good for RBI Grade B prepar...   \n",
       "51  Will a Blu Ray play on a regular DVD player? I...   \n",
       "53  What is the best/most memorable thing you've e...   \n",
       "58  I was suddenly logged off Gmail. I can't remem...   \n",
       "62  How is the new Harry Potter book 'Harry Potter...   \n",
       "\n",
       "                                            question2  \n",
       "5   I'm a triple Capricorn (Sun, Moon and ascendan...  \n",
       "7           What should I do to be a great geologist?  \n",
       "11             How can I see all my Youtube comments?  \n",
       "12            How can you make physics easy to learn?  \n",
       "13             What was your first sexual experience?  \n",
       "15  How will a Trump presidency affect the student...  \n",
       "16                      What does manipulation means?  \n",
       "18  Why do people ask Quora questions which can be...  \n",
       "20        Why are rockets and boosters painted white?  \n",
       "29  How one should know that he/she completely pre...  \n",
       "31  How can I keep my nose from getting stuffy at ...  \n",
       "32  What Game of Thrones villain would you most li...  \n",
       "38                How do I prepare for civil service?  \n",
       "48  What are some of the products made from crude ...  \n",
       "49                              How to make friends ?  \n",
       "50  How is career launcher online program for RBI ...  \n",
       "51  How can you play a Blu Ray DVD on a regular DV...  \n",
       "53  What is the most delicious dish you've ever ea...  \n",
       "58  I can't remember my Gmail password or my recov...  \n",
       "62            How bad is the new book by J.K Rowling?  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['is_duplicate']==1][:20][['question1', 'question2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply_func always MUST be defined above pool_apply, cheers!    \n",
    "apply_func = word_match_share\n",
    "train['word_share'] = pool_apply(train)\n",
    "\n",
    "apply_func = start_with_same_first_word\n",
    "train['start_with_same_world'] = pool_apply(train)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = question_length\n",
    "train['q1_char_num'] = pool_apply(train)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = question_length\n",
    "train['q2_char_num'] = pool_apply(train)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = word_count\n",
    "train['q1_word_num'] = pool_apply(train)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = word_count\n",
    "train['q2_word_num'] = pool_apply(train)\n",
    "\n",
    "apply_func = tfidf_word_match_share\n",
    "train['rfidf_share'] = pool_apply(train)\n",
    "\n",
    "train['char_difference'] = abs(train['q1_char_num'] - train['q2_char_num'])\n",
    "train['word_difference'] = abs(train['q1_word_num'] - train['q2_word_num'])\n",
    "\n",
    "apply_func = simhash_distance_seq\n",
    "train['seq_simhash_distance'] = pool_apply(train)\n",
    "\n",
    "apply_func = simhash_distance_shingle\n",
    "train['shingle_simhash_distance'] = pool_apply(train)\n",
    "\n",
    "train['avg_word_len_q1'] = train['q1_char_num'] / (train['q1_word_num'] + 10e-4)\n",
    "train['avg_word_len_q2'] = train['q2_char_num'] / (train['q2_word_num'] + 10e-4)\n",
    "train['avg_word_difference'] = abs(train['avg_word_len_q1'] - train['avg_word_len_q2'])\n",
    "\n",
    "apply_func = get_common_unigrams\n",
    "train['unigrams_common_count'] = pool_apply(train)\n",
    "\n",
    "apply_func = get_common_bigrams\n",
    "train['bigrams_common_count'] = pool_apply(train)\n",
    "\n",
    "apply_func = get_common_unigram_ratio\n",
    "train['unigrams_common_ratio'] = pool_apply(train)\n",
    "\n",
    "apply_func = get_common_bigram_ratio\n",
    "train['bigrams_common_ratio'] = pool_apply(train)\n",
    "\n",
    "apply_func = numpy_cosine\n",
    "train['cosin_sim'] = pool_apply(train, proc_num=8)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = mean_word2vec\n",
    "train['word2vec_q1_mean'] = pool_apply(train, proc_num=4)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = mean_word2vec\n",
    "train['word2vec_q2_mean'] = pool_apply(train, proc_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_pickle('../features/train.pkl')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Genearte test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "apply_func = start_with_same_first_word\n",
    "test['start_with_same_world'] = pool_apply(test)\n",
    "\n",
    "apply_func = word_match_share\n",
    "test['word_share'] = pool_apply(test)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = question_length\n",
    "test['q1_char_num'] = pool_apply(test)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = question_length\n",
    "test['q2_char_num'] = pool_apply(test)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = word_count\n",
    "test['q1_word_num'] = pool_apply(test)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = word_count\n",
    "test['q2_word_num'] = pool_apply(test)\n",
    "\n",
    "apply_func = tfidf_word_match_share\n",
    "test['rfidf_share'] = pool_apply(test)\n",
    "\n",
    "test['char_difference'] = abs(test['q1_char_num'] - test['q2_char_num'])\n",
    "test['word_difference'] = abs(test['q1_word_num'] - test['q2_word_num'])\n",
    "\n",
    "apply_func = simhash_distance_seq\n",
    "test['seq_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "apply_func = simhash_distance_shingle\n",
    "test['shingle_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "apply_func = simhash_distance_seq\n",
    "test['seq_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "apply_func = simhash_distance_shingle\n",
    "test['shingle_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "test['avg_word_len_q1'] = test['q1_char_num'] / (test['q1_word_num'] + 10e-4)\n",
    "test['avg_word_len_q2'] = test['q2_char_num'] / (test['q2_word_num'] + 10e-4)\n",
    "test['avg_word_difference'] = abs(test['avg_word_len_q1'] - test['avg_word_len_q2'])\n",
    "\n",
    "apply_func = get_common_unigrams\n",
    "test['unigrams_common_count'] = pool_apply(test)\n",
    "\n",
    "apply_func = get_common_bigrams\n",
    "test['bigrams_common_count'] = pool_apply(test)\n",
    "\n",
    "apply_func = get_common_unigram_ratio\n",
    "test['unigrams_common_ratio'] = pool_apply(test)\n",
    "\n",
    "apply_func = get_common_bigram_ratio\n",
    "test['bigrams_common_ratio'] = pool_apply(test)\n",
    "\n",
    "# This function takes a lot of RAM and it scales with the number of processes\n",
    "\n",
    "apply_func = numpy_cosine\n",
    "test['cosin_sim'] = pool_apply(test, proc_num=4) \n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = mean_word2vec\n",
    "test['word2vec_q1_mean'] = pool_apply(test, proc_num=4)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = mean_word2vec\n",
    "test['word2vec_q2_mean'] = pool_apply(test, proc_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.to_pickle('../features/test.pkl')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
