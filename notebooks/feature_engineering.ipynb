{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data with features (pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../features/train.pkl')\n",
    "test = pd.read_pickle('../features/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    for word in list(sentence):\n",
    "        if word in stops_words:\n",
    "            sentence.remove(word)  \n",
    "            \n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_dataframe(data):\n",
    "    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n",
    "    data = data.dropna(how=\"any\")\n",
    "    \n",
    "    for col in ['question1', 'question2']:\n",
    "        data[col] = data[col].apply(clean_sentence)\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_clean = clean_dataframe(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import gc\n",
    "\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "from simhash import Simhash\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus.reader.wordnet import ADJ, ADJ_SAT, ADV, NOUN, VERB\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    return data.apply(apply_func, axis=1, raw=True)\n",
    "\n",
    "def chunk(data, num):   \n",
    "    chunk_size = math.ceil(len(data) / num)\n",
    "    return [data[i*chunk_size : (i+1)*chunk_size] for i in range(num)]\n",
    "\n",
    "def pool_apply(data, proc_num=8):\n",
    "    \n",
    "    with Pool(processes=proc_num) as pool:\n",
    "        chunks = chunk(data, proc_num) \n",
    "        proccessed_chunks = list(pool.map(transform_data, chunks))\n",
    "  \n",
    "    return np.hstack(tuple(proccessed_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word_match_share(row):\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    \n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q1words[word] = 1\n",
    "            \n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q2words[word] = 1\n",
    "            \n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share(row):\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    \n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q1words[word] = 1\n",
    "            \n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q2words[word] = 1\n",
    "            \n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    with np.errstate(invalid='ignore'):\n",
    "        shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [\n",
    "            weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "        \n",
    "        total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "\n",
    "        R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "\n",
    "    return R if not math.isnan(R) else 0\n",
    "\n",
    "\n",
    "def start_with_same_first_word(row):\n",
    "    if not isinstance(row['question1'], str) or not isinstance(row['question2'], str):\n",
    "        return 0\n",
    "    \n",
    "    first_word_q1 = row['question1'].split()[0].lower()\n",
    "    first_word_q2 = row['question2'].split()[0].lower()\n",
    "    \n",
    "    return 1 if first_word_q1 == first_word_q2 else 0\n",
    "\n",
    "def question_length(row):\n",
    "    question = row[feature]\n",
    "    return len(question) if isinstance(question, str) else 0\n",
    "\n",
    "def word_count(row):\n",
    "    question = row[feature]\n",
    "    return len(question.split()) if isinstance(question, str) else 0\n",
    "\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "    \n",
    "def simhash_distance_seq(row):\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    if not isinstance(q1, str) or not isinstance(q2, str):\n",
    "        return 0\n",
    "\n",
    "    return Simhash(q1).distance(Simhash(q2))\n",
    "\n",
    "def simhash_distance_shingle(row):\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    if not isinstance(q1, str) or not isinstance(q2, str):\n",
    "        return 0\n",
    "    \n",
    "    q1_shingles = get_singles(q1)\n",
    "    q2_shingles = get_singles(q2)\n",
    "    \n",
    "    return Simhash(q1_shingles).distance(Simhash(q2_shingles))\n",
    "\n",
    "def get_singles(sequence, width = 3):\n",
    "    sequence = sequence.lower()\n",
    "    sequence = re.sub(r'[^\\w]+', '', sequence)\n",
    "    return [sequence[i:i + width] for i in range(max(len(sequence) - width + 1, 1))]\n",
    "\n",
    "\n",
    "\n",
    "def get_common_unigrams(row):\n",
    "    question1 = str(row['question1'])\n",
    "    question2 = str(row['question2'])\n",
    "    \n",
    "    q1_unigrams = set([i for i in nltk.ngrams(question1, 1)])\n",
    "    q2_unigrams = set([i for i in nltk.ngrams(question2, 1)])\n",
    "    return len( q1_unigrams.intersection(q2_unigrams))\n",
    "\n",
    "def get_common_unigram_ratio(row):\n",
    "    question1 = str(row['question1'])\n",
    "    question2 = str(row['question2'])\n",
    "    \n",
    "    q1_unigrams = set([i for i in nltk.ngrams(question1, 1)])\n",
    "    q2_unigrams = set([i for i in nltk.ngrams(question2, 1)])\n",
    "    unigram_count = float(row[\"unigrams_common_count\"])\n",
    "               \n",
    "    return  unigram_count / max(len(q1_unigrams.union(q2_unigrams)),1)\n",
    "\n",
    "def get_common_bigrams(row):\n",
    "    question1 = str(row['question1'])\n",
    "    question2 = str(row['question2'])\n",
    "    \n",
    "    q1_bigrams = set([i for i in nltk.ngrams(question1, 2)])\n",
    "    q2_bigrams = set([i for i in nltk.ngrams(question2, 2)])\n",
    "    return len(q1_bigrams.intersection(q2_bigrams))\n",
    "\n",
    "def get_common_bigram_ratio(row):\n",
    "    question1 = str(row['question1'])\n",
    "    question2 = str(row['question2'])\n",
    "    \n",
    "    q1_bigrams = set([i for i in nltk.ngrams(question1, 2)])\n",
    "    q2_bigrams = set([i for i in nltk.ngrams(question2, 2)])\n",
    "    bigram_count = float(row[\"bigrams_common_count\"])\n",
    "               \n",
    "    return  bigram_count / max(len(q1_bigrams.union(q2_bigrams)),1)\n",
    "\n",
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for col in ['question1', 'question2']:\n",
    "        for sentence in data[col].iteritems():\n",
    "            word_list = sentence[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "            \n",
    "    return corpus\n",
    "\n",
    "def question_to_word2vec(question_string, word2vec_model):\n",
    "    \"\"\"\n",
    "    Given question string, returns word2vec vector of the questions tring\n",
    "    :param question_string : The given question as a string.\n",
    "    \"\"\"\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    if not isinstance(question_string, str):\n",
    "        return 0\n",
    "    \n",
    "    words = word_tokenize(question_string)[:-1]\n",
    "    non_stop_words = []\n",
    "    for word in words:\n",
    "        if word.lower().strip('-') not in stops_words:\n",
    "            word = WordNetLemmatizer().lemmatize(word, NOUN)\n",
    "            \n",
    "            if word.lower() in word2vec_model.wv:\n",
    "                non_stop_words.append(word.lower().strip('-'))\n",
    "            \n",
    "    if len(non_stop_words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    vectors = [word2vec_model.wv[word] for word in non_stop_words]\n",
    "    vector = sum(vectors)/float(len(non_stop_words))\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def numpy_cosine(row):\n",
    "    \"\"\"\n",
    "    Cosine similarity between q1 and q2 question instances using their vectors\n",
    "    :return: similarity between q1 and q2\n",
    "    \"\"\"\n",
    "    q1, q2 = row['question1'], row['question2']\n",
    "    q1_vec, q2_vec = question_to_word2vec(q1, word2vec_model), question_to_word2vec(q2, word2vec_model)\n",
    "    \n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cosine_similarity = np.dot(q1_vec, q2_vec) / (np.linalg.norm(q1_vec) * np.linalg.norm(q2_vec))\n",
    "    \n",
    "    return cosine_similarity if isinstance(cosine_similarity, np.float32) else 0.0\n",
    "\n",
    "\n",
    "def mean_word2vec(row):\n",
    "    question = row[feature]\n",
    "    return np.mean(question_to_word2vec(question, word2vec_model))\n",
    "\n",
    "def noun_count(row):\n",
    "    question = row[feature]\n",
    "    pos_tags = pos_tag(question)\n",
    "    counts = Counter(tag for word,tag in pos_tags)\n",
    "    return counts[word_type] if word_type in counts else 0\n",
    "\n",
    "# Run clean data in the first part of the notebook to generate the train_clean dataset\n",
    "corpus = build_corpus(train_clean)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.42433310e-01,  -2.21893996e-01,  -1.80049074e+00,\n",
       "         2.92410684e+00,   1.74965656e+00,   2.44394803e+00,\n",
       "         1.17470133e+00,   5.35335112e-03,   1.61260962e+00,\n",
       "        -1.28353417e+00,  -9.53978717e-01,  -1.79907894e+00,\n",
       "        -2.51967549e+00,  -3.13276672e+00,  -3.96536684e+00,\n",
       "         1.60808218e+00,  -3.67219138e+00,   4.66184378e+00,\n",
       "         5.95021963e-01,   1.47871447e+00,  -1.62197936e+00,\n",
       "        -1.27594602e+00,   1.55187845e+00,   8.01879406e-01,\n",
       "        -5.23361981e-01,   1.02736795e+00,   1.88833916e+00,\n",
       "         1.09494722e+00,   1.00037932e-01,   9.87873971e-01,\n",
       "        -1.34864318e+00,  -2.27866173e+00,   8.98640513e-01,\n",
       "        -1.05025792e+00,  -1.71187305e+00,  -5.54719090e-01,\n",
       "         7.96048343e-01,   3.03651043e-03,   3.31802368e+00,\n",
       "         1.19908549e-01,   9.71411526e-01,   1.08135152e+00,\n",
       "        -1.17547739e+00,   6.95550919e-01,  -3.79771799e-01,\n",
       "        -8.89537036e-01,  -6.29661202e-01,   8.47659409e-01,\n",
       "        -2.33747602e-01,  -4.23385382e-01,   6.83804512e-01,\n",
       "        -8.71241987e-01,   1.35975361e+00,  -6.62091672e-01,\n",
       "         3.14228237e-01,  -1.50612459e-01,  -2.51705909e+00,\n",
       "         2.28463244e+00,   1.53936839e+00,  -1.09293319e-01,\n",
       "        -6.91880047e-01,  -9.91146505e-01,   3.76111341e+00,\n",
       "         2.13336444e+00,   6.78030133e-01,  -1.70651063e-01,\n",
       "        -1.12177086e+00,  -1.13727307e+00,   1.60881746e+00,\n",
       "         6.77342713e-01,   1.80114388e-01,  -3.35177213e-01,\n",
       "        -1.55697000e+00,  -7.11602330e-01,  -4.71638829e-01,\n",
       "         3.39475423e-01,  -4.11863178e-01,   1.06842077e+00,\n",
       "         5.96168041e-01,   1.46669495e+00,  -3.13104367e+00,\n",
       "         1.80598319e-01,  -4.98651061e-03,  -2.96812677e+00,\n",
       "        -2.80973244e+00,   6.92381620e-01,  -6.48458362e-01,\n",
       "        -6.39215291e-01,   1.25855815e+00,  -1.05727565e+00,\n",
       "        -9.63148296e-01,  -1.07383668e+00,   1.46310711e+00,\n",
       "         1.29570806e+00,   8.15780775e-04,  -1.89332628e+00,\n",
       "         1.46529746e+00,   1.80872858e+00,  -6.19652927e-01,\n",
       "         1.42362440e+00], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=500,\n",
    "                                   workers=8, seed=SEED, batch_words=5000)\n",
    "word2vec_model.wv['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eps = 5000 \n",
    "train_qs = pd.Series(train['question1'].tolist() + train['question2'].tolist()).astype(str)\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What would a Trump presidency mean for current...</td>\n",
       "      <td>How will a Trump presidency affect the student...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What does manipulation mean?</td>\n",
       "      <td>What does manipulation means?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Why are so many Quora users posting questions ...</td>\n",
       "      <td>Why do people ask Quora questions which can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Why do rockets look white?</td>\n",
       "      <td>Why are rockets and boosters painted white?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>How should I prepare for CA final law?</td>\n",
       "      <td>How one should know that he/she completely pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What are some special cares for someone with a...</td>\n",
       "      <td>How can I keep my nose from getting stuffy at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>What Game of Thrones villain would be the most...</td>\n",
       "      <td>What Game of Thrones villain would you most li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>How do we prepare for UPSC?</td>\n",
       "      <td>How do I prepare for civil service?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>What are some examples of products that can be...</td>\n",
       "      <td>What are some of the products made from crude ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>How do I make friends.</td>\n",
       "      <td>How to make friends ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Is Career Launcher good for RBI Grade B prepar...</td>\n",
       "      <td>How is career launcher online program for RBI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Will a Blu Ray play on a regular DVD player? I...</td>\n",
       "      <td>How can you play a Blu Ray DVD on a regular DV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>What is the best/most memorable thing you've e...</td>\n",
       "      <td>What is the most delicious dish you've ever ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>I was suddenly logged off Gmail. I can't remem...</td>\n",
       "      <td>I can't remember my Gmail password or my recov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>How is the new Harry Potter book 'Harry Potter...</td>\n",
       "      <td>How bad is the new book by J.K Rowling?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question1  \\\n",
       "5   Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "7                      How can I be a good geologist?   \n",
       "11        How do I read and find my YouTube comments?   \n",
       "12               What can make Physics easy to learn?   \n",
       "13        What was your first sexual experience like?   \n",
       "15  What would a Trump presidency mean for current...   \n",
       "16                       What does manipulation mean?   \n",
       "18  Why are so many Quora users posting questions ...   \n",
       "20                         Why do rockets look white?   \n",
       "29             How should I prepare for CA final law?   \n",
       "31  What are some special cares for someone with a...   \n",
       "32  What Game of Thrones villain would be the most...   \n",
       "38                        How do we prepare for UPSC?   \n",
       "48  What are some examples of products that can be...   \n",
       "49                             How do I make friends.   \n",
       "50  Is Career Launcher good for RBI Grade B prepar...   \n",
       "51  Will a Blu Ray play on a regular DVD player? I...   \n",
       "53  What is the best/most memorable thing you've e...   \n",
       "58  I was suddenly logged off Gmail. I can't remem...   \n",
       "62  How is the new Harry Potter book 'Harry Potter...   \n",
       "\n",
       "                                            question2  \n",
       "5   I'm a triple Capricorn (Sun, Moon and ascendan...  \n",
       "7           What should I do to be a great geologist?  \n",
       "11             How can I see all my Youtube comments?  \n",
       "12            How can you make physics easy to learn?  \n",
       "13             What was your first sexual experience?  \n",
       "15  How will a Trump presidency affect the student...  \n",
       "16                      What does manipulation means?  \n",
       "18  Why do people ask Quora questions which can be...  \n",
       "20        Why are rockets and boosters painted white?  \n",
       "29  How one should know that he/she completely pre...  \n",
       "31  How can I keep my nose from getting stuffy at ...  \n",
       "32  What Game of Thrones villain would you most li...  \n",
       "38                How do I prepare for civil service?  \n",
       "48  What are some of the products made from crude ...  \n",
       "49                              How to make friends ?  \n",
       "50  How is career launcher online program for RBI ...  \n",
       "51  How can you play a Blu Ray DVD on a regular DV...  \n",
       "53  What is the most delicious dish you've ever ea...  \n",
       "58  I can't remember my Gmail password or my recov...  \n",
       "62            How bad is the new book by J.K Rowling?  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['is_duplicate']==1][:20][['question1', 'question2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply_func always MUST be defined above pool_apply, cheers!    \n",
    "apply_func = word_count\n",
    "for word_type in ['NN', 'RB', 'VB', 'DT', 'JJ', 'FW', 'RP', 'SYM']:  \n",
    "    feature = 'question1'\n",
    "    q1_feature_name = 'q1_{tag}_count'.format(tag=word_type)\n",
    "    train[q1_feature_name] = pool_apply(train)\n",
    "\n",
    "    feature = 'question2'\n",
    "    q2_feature_name = 'q2_{tag}_count'.format(tag=word_type)\n",
    "    train[q2_feature_name] = pool_apply(train)\n",
    "\n",
    "    train['{tag}_diff'.format(tag=word_type)] = abs(train[q1_feature_name] - train[q2_feature_name])\n",
    "\n",
    "apply_func = word_match_share\n",
    "train['word_share'] = pool_apply(train)\n",
    "\n",
    "apply_func = start_with_same_first_word\n",
    "train['start_with_same_world'] = pool_apply(train)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = question_length\n",
    "train['q1_char_num'] = pool_apply(train)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = question_length\n",
    "train['q2_char_num'] = pool_apply(train)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = word_count\n",
    "train['q1_word_num'] = pool_apply(train)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = word_count\n",
    "train['q2_word_num'] = pool_apply(train)\n",
    "\n",
    "apply_func = tfidf_word_match_share\n",
    "train['rfidf_share'] = pool_apply(train)\n",
    "\n",
    "train['char_difference'] = abs(train['q1_char_num'] - train['q2_char_num'])\n",
    "train['word_difference'] = abs(train['q1_word_num'] - train['q2_word_num'])\n",
    "\n",
    "apply_func = simhash_distance_seq\n",
    "train['seq_simhash_distance'] = pool_apply(train)\n",
    "\n",
    "apply_func = simhash_distance_shingle\n",
    "train['shingle_simhash_distance'] = pool_apply(train)\n",
    "\n",
    "train['avg_word_len_q1'] = train['q1_char_num'] / (train['q1_word_num'] + 10e-4)\n",
    "train['avg_word_len_q2'] = train['q2_char_num'] / (train['q2_word_num'] + 10e-4)\n",
    "train['avg_word_difference'] = abs(train['avg_word_len_q1'] - train['avg_word_len_q2'])\n",
    "\n",
    "apply_func = get_common_unigrams\n",
    "train['unigrams_common_count'] = pool_apply(train)\n",
    "\n",
    "apply_func = get_common_bigrams\n",
    "train['bigrams_common_count'] = pool_apply(train)\n",
    "\n",
    "apply_func = get_common_unigram_ratio\n",
    "train['unigrams_common_ratio'] = pool_apply(train)\n",
    "\n",
    "apply_func = get_common_bigram_ratio\n",
    "train['bigrams_common_ratio'] = pool_apply(train)\n",
    "\n",
    "apply_func = numpy_cosine\n",
    "train['cosin_sim'] = pool_apply(train, proc_num=8)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = mean_word2vec\n",
    "train['word2vec_q1_mean'] = pool_apply(train, proc_num=4)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = mean_word2vec\n",
    "train['word2vec_q2_mean'] = pool_apply(train, proc_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.to_pickle('../features/train_new.pkl')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Genearte test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply_func always MUST be defined above pool_apply, cheers!    \n",
    "apply_func = word_count\n",
    "for word_type in  ['NN', 'RB', 'VB', 'DT', 'JJ', 'FW', 'RP', 'SYM']:  \n",
    "    feature = 'question1'\n",
    "    q1_feature_name = 'q1_{tag}_count'.format(tag=word_type)\n",
    "    test[q1_feature_name] = pool_apply(test)\n",
    "\n",
    "    feature = 'question2'\n",
    "    q2_feature_name = 'q2_{tag}_count'.format(tag=word_type)\n",
    "    test[q2_feature_name] = pool_apply(test)\n",
    "\n",
    "    test['{tag}_diff'.format(tag=word_type)] = abs(test[q1_feature_name] - test[q2_feature_name])\n",
    "\n",
    "apply_func = start_with_same_first_word\n",
    "test['start_with_same_world'] = pool_apply(test)\n",
    "\n",
    "apply_func = word_match_share\n",
    "test['word_share'] = pool_apply(test)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = question_length\n",
    "test['q1_char_num'] = pool_apply(test)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = question_length\n",
    "test['q2_char_num'] = pool_apply(test)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = word_count\n",
    "test['q1_word_num'] = pool_apply(test)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = word_count\n",
    "test['q2_word_num'] = pool_apply(test)\n",
    "\n",
    "apply_func = tfidf_word_match_share\n",
    "test['rfidf_share'] = pool_apply(test)\n",
    "\n",
    "test['char_difference'] = abs(test['q1_char_num'] - test['q2_char_num'])\n",
    "test['word_difference'] = abs(test['q1_word_num'] - test['q2_word_num'])\n",
    "\n",
    "apply_func = simhash_distance_seq\n",
    "test['seq_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "apply_func = simhash_distance_shingle\n",
    "test['shingle_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "apply_func = simhash_distance_seq\n",
    "test['seq_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "apply_func = simhash_distance_shingle\n",
    "test['shingle_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "test['avg_word_len_q1'] = test['q1_char_num'] / (test['q1_word_num'] + 10e-4)\n",
    "test['avg_word_len_q2'] = test['q2_char_num'] / (test['q2_word_num'] + 10e-4)\n",
    "test['avg_word_difference'] = abs(test['avg_word_len_q1'] - test['avg_word_len_q2'])\n",
    "\n",
    "apply_func = get_common_unigrams\n",
    "test['unigrams_common_count'] = pool_apply(test)\n",
    "\n",
    "apply_func = get_common_bigrams\n",
    "test['bigrams_common_count'] = pool_apply(test)\n",
    "\n",
    "apply_func = get_common_unigram_ratio\n",
    "test['unigrams_common_ratio'] = pool_apply(test)\n",
    "\n",
    "apply_func = get_common_bigram_ratio\n",
    "test['bigrams_common_ratio'] = pool_apply(test)\n",
    "\n",
    "# This function takes a lot of RAM and it scales with the number of processes\n",
    "\n",
    "apply_func = numpy_cosine\n",
    "test['cosin_sim'] = pool_apply(test, proc_num=4) \n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = mean_word2vec\n",
    "test['word2vec_q1_mean'] = pool_apply(test, proc_num=4)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = mean_word2vec\n",
    "test['word2vec_q2_mean'] = pool_apply(test, proc_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test.to_pickle('../features/test_new.pkl')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
