{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load features\n",
    "To load the features you first have to create them, run the notebook feature_engineering. Beware it takes about 2-3 hours to run so save your features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../features/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_pickle('../features/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Consts\n",
    "Always use constant SEED otherwise the experiment is not reproducable, in that case why are we doing it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = ['cosin_sim', 'word_share', 'q1_char_num', 'q1_word_num', 'q2_char_num', 'q2_word_num',\n",
    "            'start_with_same_world', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "           'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1', 'avg_word_len_q2',\n",
    "           'avg_word_difference', 'unigrams_common_count', 'bigrams_common_count', 'unigrams_common_ratio',\n",
    "           'bigrams_common_ratio', 'word2vec_q1_mean', 'word2vec_q2_mean']\n",
    "\n",
    "target = 'is_duplicate'\n",
    "\n",
    "X = train[features]\n",
    "y = train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "\n",
    "max_len = 40\n",
    "tokenizer.fit_on_texts(list(train.question1.values.astype(str)) + list(train.question2.values.astype(str)))\n",
    "\n",
    "x1 = tokenizer.texts_to_sequences(train.question1.values.astype(str))\n",
    "x1 = sequence.pad_sequences(x1, maxlen=max_len)\n",
    "\n",
    "x2 = tokenizer.texts_to_sequences(train.question2.values.astype(str))\n",
    "x2 = sequence.pad_sequences(x2, maxlen=max_len)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 40) (404290, 21)\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   383,     8,    35],\n",
       "       [    0,     0,     0, ..., 13893,     5,  4572],\n",
       "       [    0,     0,     0, ...,   146,     6,  2775],\n",
       "       ..., \n",
       "       [    0,     0,     0, ...,     3,    49,  4401],\n",
       "       [    0,     0,     0, ...,    32,    82,   234],\n",
       "       [    0,     0,     0, ...,   155,    29,  4549]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Oversampling\n",
    "Oversampling leads to local validation score not matching the score from public LB on kaggle. Models with oversampling usually perform a bit better, but due to scores not maching if possible better not use it.\n",
    "\n",
    "The idea for oversampling came from Kaggle (https://www.kaggle.com/davidthaler/quora-question-pairs/how-many-1-s-are-in-the-public-lb) because the training and test set do not have the same distribution of dublicate questions. The train set has around 37% of duplicates while the private test set has 16.5% but the problem is that we only see the 35% of the prive test set. Final results are calculate on the remaining 65%, what if the distribution of the 35% set doe not match the other 65%, in that case oversampling while increasing the public LB score currently would yield in overfitting the score and poor results in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def oversample(X, y, rate=0.165):\n",
    "    pos_train = X[y == 1]\n",
    "    neg_train = X[y == 0]\n",
    "\n",
    "    # Now we oversample the negative class\n",
    "    # There is likely a much more elegant way to do this...\n",
    "    p = 0.165\n",
    "    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_train = pd.concat([neg_train, neg_train])\n",
    "        scale -=1\n",
    "    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "    print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "    X = pd.concat([pos_train, neg_train])\n",
    "    y = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "y_untouched = y\n",
    "\n",
    "x1 = pd.DataFrame(x1) \n",
    "x2 = pd.DataFrame(x2) \n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "X, y = oversample(X, y_untouched)\n",
    "x1, y = oversample(x1, y_untouched)\n",
    "x2, y = oversample(x2, y_untouched)\n",
    "\n",
    "X = np.array(X)\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "\n",
    "print(len(X), len(x1), len(x2), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Normalization\n",
    "Normalization helps but only if X is normalized, normalizing x1 and x2 does not allow the model to converge and pass the val_logloss of 0.42 -> bad. So far it seems that StandardScaler applied only on X does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaler_X = MinMaxScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaler_x1 = MinMaxScaler()\n",
    "x1 = scaler_x1.fit_transform(x1)\n",
    "\n",
    "scaler_x2 = MinMaxScaler()\n",
    "x2 = scaler_x2.fit_transform(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "#from keras.layers.merge import Concatenate\n",
    "from keras.layers import Merge\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Dense, BatchNormalization, TimeDistributed\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop, Adamax, Adagrad, Nadam\n",
    "from keras.activations import elu\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_q1 = Sequential()\n",
    "model_q1.add(Embedding(len(word_index) + 1, 64, input_length=max_len, input_shape=(x1.shape[1],)))\n",
    "model_q1.add(GRU(64, recurrent_dropout=0.2, dropout=0.2, return_sequences=True))\n",
    "model_q1.add(GRU(64, recurrent_dropout=0.2, dropout=0.2, return_sequences=False))\n",
    "\n",
    "model_q2 = Sequential()\n",
    "model_q2.add(Embedding(len(word_index) + 1, 64, input_length=max_len, input_shape=(x2.shape[1],)))\n",
    "model_q2.add(GRU(64, recurrent_dropout=0.2, dropout=0.2, return_sequences=True))\n",
    "model_q2.add(GRU(64, recurrent_dropout=0.2, dropout=0.2, return_sequences=False))\n",
    "\n",
    "model_features = Sequential()\n",
    "model_features.add(Dense(128, input_shape=(X.shape[1],), activation=elu))\n",
    "model_features.add(BatchNormalization())\n",
    "\n",
    "model_features.add(Dense(256, activation=elu))\n",
    "model_features.add(BatchNormalization())\n",
    "\n",
    "model_features.add(Dense(512, activation=elu))\n",
    "model_features.add(Dropout(0.2))\n",
    "model_features.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:2: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "merged_model = Sequential()\n",
    "merged_model.add(Merge([model_q1, model_q2, model_features], mode = 'concat'))\n",
    "#merged_model.add(Concatenate([model_q1, model_q2, model_features]))\n",
    "merged_model.add(BatchNormalization())\n",
    "\n",
    "merged_model.add(Dense(128, activation=elu))\n",
    "merged_model.add(BatchNormalization())\n",
    "\n",
    "merged_model.add(Dense(256, activation=elu))\n",
    "merged_model.add(BatchNormalization())\n",
    "\n",
    "merged_model.add(Dense(512, activation=elu))\n",
    "model_features.add(Dropout(0.2))\n",
    "merged_model.add(BatchNormalization())\n",
    "\n",
    "merged_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adamax(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#merged_model.load_weights('MergeNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# keras_logger = keras.callbacks.TensorBoard(log_dir='../notebooks/tensor_logs/mergnet5',\n",
    "#                                            histogram_freq=1, write_graph=True, write_images=True)\n",
    "\n",
    "# keras_logger.set_model(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4be05a93f1744bf95c1e8c90a8f6042"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cfab2699a94992abf75b97513f0461"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103dc39cb3e046b78476cbb1a8de35d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cf406a76b941e4845c6c5d1d4b14a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1dccdc80dc74033b860f257364c1f0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dddb27bfa8467ea55aa038eccab85f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e876263ccce4d0ead85c827dd0821cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d92ec83582b472ca1bdac42c2f0f89b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71236a64f10e4090bda6a83005fa8bd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_model.fit([x1, x2, X],\n",
    "          y,\n",
    "          batch_size=256,\n",
    "          epochs=50,\n",
    "          verbose=0,\n",
    "          validation_split=0.1,\n",
    "          #callbacks=[TQDMNotebookCallback(), keras_logger])\n",
    "          callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#merged_model.save_weights('MergeNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# scores = model.evaluate(X, y, verbose=0, batch_size=4096 * 8)\n",
    "\n",
    "# print(\"Model validation accuracy: %.2f\" % (scores[1]*100))\n",
    "# print(\"Model validation loss: %.4f\" % (scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate submission\n",
    "\n",
    "Run the version with the chunker if you don't have more then 24GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def chunker(collection, chunk_size=300000):\n",
    "    chunk_num = math.ceil(collection.shape[0] / float(chunk_size))\n",
    "    for i in range(chunk_num):\n",
    "        yield collection[chunk_size*i : chunk_size*(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for q1, q2, X_test_row in zip(\n",
    "    chunker(test.question1), chunker(test.question2), chunker(X_test)\n",
    "):\n",
    "    print('%d / %d' % (len(preds), len(X_test)))\n",
    "    x1_test_row = tokenizer.texts_to_sequences(q1.values.astype(str))\n",
    "    x1_test_row = sequence.pad_sequences(x1_test_row, maxlen=max_len)\n",
    "\n",
    "    x2_test_row = tokenizer.texts_to_sequences(q2.values.astype(str))\n",
    "    x2_test_row = sequence.pad_sequences(x2_test_row, maxlen=max_len)\n",
    "    \n",
    "    #x1_test_row = scaler_x1.transform(x1_test_row)\n",
    "    #x2_test_row = scaler_x2.transform(x2_test_row)\n",
    "\n",
    "    batch_preds = merged_model.predict([x1_test_row, x2_test_row, X_test_row], batch_size=128 * 32)\n",
    "    preds.extend(batch_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test = test[features]\n",
    "X_test = scaler_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    '../submissions/submission.csv', np.c_[range(len(preds)), preds],\n",
    "    delimiter=',', header='test_id,is_duplicate', comments='', fmt='%d,%f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
