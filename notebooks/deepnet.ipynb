{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../features/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_pickle('../features/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = ['cosin_sim', 'word_share', 'q1_char_num', 'q1_word_num', 'q2_char_num', 'q2_word_num',\n",
    "            'start_with_same_world', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "           'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1', 'avg_word_len_q2',\n",
    "           'avg_word_difference', 'unigrams_common_count', 'bigrams_common_count', 'unigrams_common_ratio',\n",
    "           'bigrams_common_ratio', 'word2vec_q1_mean', 'word2vec_q2_mean']\n",
    "\n",
    "target = 'is_duplicate'\n",
    "\n",
    "X = train[features]\n",
    "y = train[target]\n",
    "\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=10000)\n",
    "\n",
    "max_len = 40\n",
    "tokenizer.fit_on_texts(list(train.question1.values.astype(str)) + list(train.question2.values.astype(str)))\n",
    "\n",
    "x1 = tokenizer.texts_to_sequences(train.question1.values.astype(str))\n",
    "x1 = sequence.pad_sequences(x1, maxlen=max_len)\n",
    "\n",
    "x2 = tokenizer.texts_to_sequences(train.question2.values.astype(str))\n",
    "x2 = sequence.pad_sequences(x2, maxlen=max_len)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 40) (404290, 21)\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run this only if you have > 24GB of RAM!\n",
    "\n",
    "# x1_test = tokenizer.texts_to_sequences(test.question1.values.astype(str))\n",
    "# x1_test = sequence.pad_sequences(x1_test, maxlen=max_len)\n",
    "\n",
    "# x2_test = tokenizer.texts_to_sequences(test.question2.values.astype(str))\n",
    "# x2_test = sequence.pad_sequences(x2_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def oversample(X, y, rate=0.165):\n",
    "    pos_train = X[y == 1]\n",
    "    neg_train = X[y == 0]\n",
    "\n",
    "    # Now we oversample the negative class\n",
    "    # There is likely a much more elegant way to do this...\n",
    "    p = 0.165\n",
    "    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_train = pd.concat([neg_train, neg_train])\n",
    "        scale -=1\n",
    "    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "    print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "    X = pd.concat([pos_train, neg_train])\n",
    "    y = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "y_untouched = y\n",
    "\n",
    "x1 = pd.DataFrame(x1) \n",
    "x2 = pd.DataFrame(x2) \n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "X, y = oversample(X, y_untouched)\n",
    "x1, y = oversample(x1, y_untouched)\n",
    "x2, y = oversample(x2, y_untouched)\n",
    "\n",
    "X = np.array(X)\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "\n",
    "print(len(X), len(x1), len(x2), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(X)\n",
    "X_test = scaler_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler_x1 = StandardScaler()\n",
    "x1 = scaler_x1.fit_transform(x1)\n",
    "\n",
    "scaler_x2 = StandardScaler()\n",
    "x2 = scaler_x2.fit_transform(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "#from keras.layers.merge import Concatenate\n",
    "from keras.layers import Merge\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Dense, BatchNormalization\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_q1 = Sequential()\n",
    "model_q1.add(Embedding(len(word_index) + 1, 64, input_length=max_len, input_shape=(x1.shape[1],)))\n",
    "model_q1.add(GRU(64, recurrent_dropout=0.2, dropout=0.2, return_sequences=True))\n",
    "model_q1.add(GRU(64, recurrent_dropout=0.2, dropout=0.2))\n",
    "\n",
    "model_q2 = Sequential()\n",
    "model_q2.add(Embedding(len(word_index) + 1, 64, input_length=max_len, input_shape=(x1.shape[1],)))\n",
    "model_q2.add(GRU(64, recurrent_dropout=0.2, dropout=0.2, return_sequences=True,))\n",
    "model_q2.add(GRU(64, recurrent_dropout=0.2, dropout=0.2))\n",
    "\n",
    "model_features = Sequential()\n",
    "model_features.add(Dense(64, input_shape=(X.shape[1],), activation='relu'))\n",
    "model_features.add(BatchNormalization())\n",
    "\n",
    "model_features.add(Dense(128, activation='relu'))\n",
    "model_features.add(BatchNormalization())\n",
    "\n",
    "model_features.add(Dense(256, activation='relu'))\n",
    "model_features.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:2: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "merged_model = Sequential()\n",
    "merged_model.add(Merge([model_q1, model_q2, model_features], mode = 'concat'))\n",
    "#merged_model.add(Concatenate([model_q1, model_q2, model_features]))\n",
    "merged_model.add(BatchNormalization())\n",
    "\n",
    "merged_model.add(Dense(16, activation='relu'))\n",
    "merged_model.add(BatchNormalization())\n",
    "\n",
    "merged_model.add(Dense(32, activation='relu'))\n",
    "merged_model.add(BatchNormalization())\n",
    "\n",
    "merged_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merged_model.load_weights('MergeNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "keras_logger = keras.callbacks.TensorBoard(log_dir='../notebooks/tensor_logs/GRU_FC_merge',\n",
    "                                           histogram_freq=1, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0817f7a33094dea91fd0b1ad06e8949"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a89fdfb56d430498b6c5e151edf607"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c369dfe4e55476996c7b239dcf431d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "363776/|/[loss: 0.412, acc: 0.779] 100%|| 363776/363861 [09:13<00:00, 664.96it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18192f4b2fc429a8d1732301166a90a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "363776/|/[loss: 0.409, acc: 0.782] 100%|| 363776/363861 [09:18<00:00, 692.22it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcde25800514c418be9eba7f518e034"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489e79d99bb14315ac01a437671dcdb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "363776/|/[loss: 0.405, acc: 0.784] 100%|| 363776/363861 [09:23<00:00, 696.34it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c441ac508f748169ac78ea94d13e409"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_model.fit([x1, x2, X],\n",
    "          y,\n",
    "          batch_size=16 * 16,\n",
    "          epochs=7,\n",
    "          verbose=0,\n",
    "          validation_split=0.1,\n",
    "          #callbacks=[TQDMNotebookCallback(), keras_logger])\n",
    "          callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#merged_model.save_weights('MergeNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# scores = model.evaluate(X, y, verbose=0, batch_size=4096 * 8)\n",
    "\n",
    "# print(\"Model validation accuracy: %.2f\" % (scores[1]*100))\n",
    "# print(\"Model validation loss: %.4f\" % (scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate submission\n",
    "\n",
    "Run the version with the chunker if you don't have more then 24GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def chunker(collection, chunk_size=300000):\n",
    "    chunk_num = math.ceil(collection.shape[0] / float(chunk_size))\n",
    "    for i in range(chunk_num):\n",
    "        yield collection[chunk_size*i : chunk_size*(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 2345796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000 / 2345796\n",
      "600000 / 2345796\n",
      "900000 / 2345796\n",
      "1200000 / 2345796\n",
      "1500000 / 2345796\n",
      "1800000 / 2345796\n",
      "2100000 / 2345796\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for q1, q2, X_test_row in zip(\n",
    "    chunker(test.question1), chunker(test.question2), chunker(X_test)\n",
    "):\n",
    "    print('%d / %d' % (len(preds), len(X_test)))\n",
    "    x1_test_row = tokenizer.texts_to_sequences(q1.values.astype(str))\n",
    "    x1_test_row = sequence.pad_sequences(x1_test_row, maxlen=max_len)\n",
    "\n",
    "    x2_test_row = tokenizer.texts_to_sequences(q2.values.astype(str))\n",
    "    x2_test_row = sequence.pad_sequences(x2_test_row, maxlen=max_len)\n",
    "    \n",
    "    x1_test_row = scaler_x1.transform(x1_test_row)\n",
    "    x2_test_row = scaler_x2.transform(x2_test_row)\n",
    "\n",
    "    batch_preds = merged_model.predict([x1_test_row, x2_test_row, X_test_row], batch_size=128 * 32)\n",
    "    preds.extend(batch_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run this only if you have > 24GB of RAM!\n",
    "\n",
    "# x1_test = tokenizer.texts_to_sequences(test.question1.values.astype(str))\n",
    "# x1_test = sequence.pad_sequences(x1_test, maxlen=max_len)\n",
    "\n",
    "# x2_test = tokenizer.texts_to_sequences(test.question2.values.astype(str))\n",
    "# x2_test = sequence.pad_sequences(x2_test, maxlen=max_len)\n",
    "\n",
    "# preds = model.predict([x1_test, x2_test, X_test], batch_size=64 * 64)\n",
    "# print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345796\n"
     ]
    }
   ],
   "source": [
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    '../submissions/submission.csv', np.c_[range(len(preds)), preds],\n",
    "    delimiter=',', header='test_id,is_duplicate', comments='', fmt='%d,%f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
