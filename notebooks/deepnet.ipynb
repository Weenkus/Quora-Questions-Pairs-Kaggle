{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import configparser\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load features\n",
    "To load the features you first have to create them, run the notebook feature_engineering. Beware it takes about 2-3 hours to run so save your features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_old = pd.read_pickle('../features/train_new.pkl')\n",
    "print(train_old.shape)\n",
    "\n",
    "train_old = pd.read_pickle('../features/train_new.pkl')\n",
    "train_bekavac = pd.read_csv('../features/train_features_bekavac_v2.csv')\n",
    "train_magic1 = pd.read_csv('../features/train_magic_feature_v1.csv')\n",
    "train_magic2 = pd.read_csv('../features/train_magic_feature_v2.csv')\n",
    "train_magic3 = pd.read_csv('../features/train_magic_feature_v3.csv')\n",
    "abhishek_train = pd.read_csv('../features/abhishek_train_features.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "train_magic1 = train_magic1.drop('is_duplicate', 1)\n",
    "train_magic1 = train_magic1.drop('question2', 1)\n",
    "train_magic1 = train_magic1.drop('question1', 1)\n",
    "\n",
    "train_magic3 = train_magic3.drop('is_duplicate', 1)\n",
    "train_magic3 = train_magic3.drop('question2', 1)\n",
    "train_magic3 = train_magic3.drop('question1', 1)\n",
    "\n",
    "abhishek_train = abhishek_train.drop('question2', 1)\n",
    "abhishek_train = abhishek_train.drop('question1', 1)\n",
    "\n",
    "train = pd.concat([train_old, train_bekavac, train_magic1, train_magic2, train_magic3, abhishek_train], axis=1)\n",
    "\n",
    "del train_old, train_bekavac, train_magic1, train_magic2, train_magic3, abhishek_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_old = pd.read_pickle('../../features/test_new.pkl')\n",
    "test_bekavac = pd.read_csv('../../features/test_features_bekavac_v2.csv')\n",
    "test_magic1 = pd.read_csv('../../features/test_magic_feature_v1.csv')\n",
    "test_magic3 = pd.read_csv('../../features/test_magic_feature_v3.csv')\n",
    "\n",
    "test_magic1 = test_magic1.drop('is_duplicate', 1)\n",
    "test_magic1 = test_magic1.drop('question2', 1)\n",
    "test_magic1 = test_magic1.drop('question1', 1)\n",
    "\n",
    "test_magic3 = test_magic3.drop('question2', 1)\n",
    "test_magic3 = test_magic3.drop('question1', 1)\n",
    "\n",
    "abhishek_test = abhishek_test.drop('question2', 1)\n",
    "abhishek_test = abhishek_test.drop('question1', 1)\n",
    "\n",
    "test_magic2 = pd.read_csv('../../features/test_magic_feature_v2.csv')\n",
    "\n",
    "test = pd.concat([test_old, test_bekavac, test_magic1, test_magic2, test_magic3, abhishek_test], axis=1)\n",
    "\n",
    "del test_old, test_bekavac, test_magic1, test_magic2, test_magic3, abhishek_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Consts\n",
    "Always use constant SEED otherwise the experiment is not reproducable, in that case why are we doing it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "NUM_WORDS = 70000\n",
    "SEQ_MAX_LEN = 30\n",
    "EMBEDDING_DIM = 100 # 50, 100, 200 or 300\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = list(set([\n",
    "    'compression_ratio_feature', 'document_pos_similarity_10_feature',\n",
    "       'document_pos_similarity_3_feature',\n",
    "       'document_pos_similarity_5_feature',\n",
    "       'document_pos_similarity_7_feature',\n",
    "       'document_pos_similarity_all_feature', 'email_similarity_feature',\n",
    "       'entities_similarity_feature', 'filtered_cosine_similarity_feature',\n",
    "       'first_word_similarity_feature', 'heads_similarity_feature',\n",
    "       'interrogative_match_feature', 'last_word_similarity_feature',\n",
    "       'lemma_edit_distance_feature', 'non_alphanumeric_similarity_feature',\n",
    "    \n",
    "       'number_of_children_similarity_5_feature', 'numbers_similarity_feature',\n",
    "       'objects_similarity_feature', 'question_length_similarity_feature',\n",
    "       'roots_similarity_feature', 'spacy_similarity_feature',\n",
    "       'subject_verb_inversion_similarity_feature',\n",
    "       'subjects_similarity_feature',\n",
    "       'unigram_idf_cutoff_similarity_10_feature',\n",
    "       'unigram_idf_cutoff_similarity_12.5_feature',\n",
    "    \n",
    "       'unigram_idf_cutoff_similarity_15_feature',\n",
    "       'unigram_idf_cutoff_similarity_1_feature',\n",
    "       'unigram_idf_cutoff_similarity_5_feature',\n",
    "       'unigram_idf_cutoff_similarity_7.5_feature',\n",
    "       'unigram_idf_mean_difference_feature', 'url_similarity_feature',\n",
    "\n",
    "     'q1_q2_wm_ratio', 'len_char_q1', 'len_char_q2', 'len_word_q1', 'len_word_q2',\n",
    "    'common_words', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio',\n",
    "    \n",
    "     'fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio', 'fuzz_token_sort_ratio',\n",
    "    \n",
    "#     'wmd', \n",
    "    #'norm_wmd',\n",
    "    'cosine_distance',\n",
    "    'cityblock_distance',\n",
    "    'jaccard_distance',\n",
    "    \n",
    "     'canberra_distance', 'euclidean_distance', 'minkowski_distance', 'braycurtis_distance',\n",
    "     'skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec',\n",
    "\n",
    "    'q1_freq', 'q2_freq', 'q1_q2_intersect', 'word_share',\n",
    "    'start_with_same_world', 'q1_char_num', 'q2_char_num', 'q1_word_num',\n",
    "    'q2_word_num', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "    'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1',\n",
    "    'avg_word_len_q2', 'avg_word_difference', 'unigrams_common_count',\n",
    "    'bigrams_common_count', 'unigrams_common_ratio', 'bigrams_common_ratio',\n",
    "    'cosin_sim', 'word2vec_q1_mean', 'word2vec_q2_mean', 'q1_NN_count',\n",
    "    'q2_NN_count', 'NN_diff', 'q1_RB_count', 'q2_RB_count', 'RB_diff',\n",
    "    'q1_VB_count', 'q2_VB_count', 'VB_diff', 'q1_DT_count', 'q2_DT_count',\n",
    "    'DT_diff', 'q1_JJ_count', 'q2_JJ_count', 'JJ_diff', 'q1_FW_count',\n",
    "    'q2_FW_count', 'FW_diff', 'q1_RP_count', 'q2_RP_count', 'RP_diff',\n",
    "    'q1_SYM_count', 'q2_SYM_count', 'SYM_diff']))\n",
    "\n",
    "# features = list(set([\n",
    "#     'compression_ratio_feature', 'document_pos_similarity_10_feature',\n",
    "#        'document_pos_similarity_3_feature',\n",
    "#        'document_pos_similarity_5_feature',\n",
    "#        'document_pos_similarity_7_feature',\n",
    "#        'document_pos_similarity_all_feature', 'email_similarity_feature',\n",
    "#        'entities_similarity_feature', 'filtered_cosine_similarity_feature',\n",
    "#        'first_word_similarity_feature', 'heads_similarity_feature',\n",
    "#        'interrogative_match_feature', 'last_word_similarity_feature',\n",
    "#        'lemma_edit_distance_feature', 'non_alphanumeric_similarity_feature',\n",
    "#        'number_of_children_similarity_5_feature', 'numbers_similarity_feature',\n",
    "#        'objects_similarity_feature', 'question_length_similarity_feature',\n",
    "#        'roots_similarity_feature', 'spacy_similarity_feature',\n",
    "#        'subject_verb_inversion_similarity_feature',\n",
    "#        'subjects_similarity_feature',\n",
    "#        'unigram_idf_cutoff_similarity_10_feature',\n",
    "#        'unigram_idf_cutoff_similarity_12.5_feature',\n",
    "#        'unigram_idf_cutoff_similarity_15_feature',\n",
    "#        'unigram_idf_cutoff_similarity_1_feature',\n",
    "#        'unigram_idf_cutoff_similarity_5_feature',\n",
    "#        'unigram_idf_cutoff_similarity_7.5_feature',\n",
    "#        'unigram_idf_mean_difference_feature', 'url_similarity_feature',\n",
    "\n",
    "#     'q1_q2_wm_ratio', 'len_char_q1', 'len_char_q2', 'len_word_q1', 'len_word_q2',\n",
    "#     'common_words', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio',\n",
    "#     'fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio', 'fuzz_token_sort_ratio',\n",
    "#     'wmd', 'norm_wmd', 'cosine_distance', 'cityblock_distance', 'jaccard_distance',\n",
    "#     'canberra_distance', 'euclidean_distance', 'minkowski_distance', 'braycurtis_distance',\n",
    "#     'skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec',\n",
    "\n",
    "#     'q1_freq', 'q2_freq', 'q1_q2_intersect', 'word_share',\n",
    "#     'start_with_same_world', 'q1_char_num', 'q2_char_num', 'q1_word_num',\n",
    "#     'q2_word_num', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "#     'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1',\n",
    "#     'avg_word_len_q2', 'avg_word_difference', 'unigrams_common_count',\n",
    "#     'bigrams_common_count', 'unigrams_common_ratio', 'bigrams_common_ratio',\n",
    "#     'cosin_sim', 'word2vec_q1_mean', 'word2vec_q2_mean', 'q1_NN_count',\n",
    "#     'q2_NN_count', 'NN_diff', 'q1_RB_count', 'q2_RB_count', 'RB_diff',\n",
    "#     'q1_VB_count', 'q2_VB_count', 'VB_diff', 'q1_DT_count', 'q2_DT_count',\n",
    "#     'DT_diff', 'q1_JJ_count', 'q2_JJ_count', 'JJ_diff', 'q1_FW_count',\n",
    "#     'q2_FW_count', 'FW_diff', 'q1_RP_count', 'q2_RP_count', 'RP_diff',\n",
    "#     'q1_SYM_count', 'q2_SYM_count', 'SYM_diff']))\n",
    "\n",
    "# features = ['q1_freq', 'q2_freq', 'q1_q2_intersect',\n",
    "#             'word_share',\n",
    "#        'start_with_same_world', 'q1_char_num', 'q2_char_num', 'q1_word_num',\n",
    "#        'q2_word_num', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "#        'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1',\n",
    "#        'avg_word_len_q2', 'avg_word_difference', 'unigrams_common_count',\n",
    "#        'bigrams_common_count', 'unigrams_common_ratio', 'bigrams_common_ratio',\n",
    "#        'cosin_sim', 'word2vec_q1_mean', 'word2vec_q2_mean', 'q1_NN_count',\n",
    "#        'q2_NN_count', 'NN_diff', 'q1_RB_count', 'q2_RB_count', 'RB_diff',\n",
    "#        'q1_VB_count', 'q2_VB_count', 'VB_diff', 'q1_DT_count', 'q2_DT_count',\n",
    "#        'DT_diff', 'q1_JJ_count', 'q2_JJ_count', 'JJ_diff', 'q1_FW_count',\n",
    "#        'q2_FW_count', 'FW_diff', 'q1_RP_count', 'q2_RP_count', 'RP_diff',\n",
    "#        'q1_SYM_count', 'q2_SYM_count', 'SYM_diff',\n",
    "#        'document_pos_similarity_10_feature',\n",
    "#        'document_pos_similarity_3_feature', 'entities_similarity_feature',\n",
    "#        'heads_similarity_feature', 'interrogative_match_feature',\n",
    "#        'non_alphanumeric_similarity_feature',\n",
    "#        'number_of_children_similarity_5_feature', 'numbers_similarity_feature',\n",
    "#        'objects_similarity_feature', 'roots_similarity_feature',\n",
    "#        'spacy_similarity_feature', 'subject_verb_inversion_similarity_feature',\n",
    "#        'subjects_similarity_feature',\n",
    "#        'unigram_idf_cutoff_similarity_10_feature',\n",
    "#        'unigram_idf_cutoff_similarity_15_feature',\n",
    "#        'unigram_idf_cutoff_similarity_5_feature',\n",
    "#        'unigram_idf_mean_difference_feature']\n",
    "\n",
    "# features = ['q1_hash', 'q2_hash', 'q1_freq', 'q2_freq', 'q1_q2_intersect',\n",
    "#             'word_share',\n",
    "#        'start_with_same_world', 'q1_char_num', 'q2_char_num', 'q1_word_num',\n",
    "#        'q2_word_num', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "#        'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1',\n",
    "#        'avg_word_len_q2', 'avg_word_difference', 'unigrams_common_count',\n",
    "#        'bigrams_common_count', 'unigrams_common_ratio', 'bigrams_common_ratio',\n",
    "#        'cosin_sim', 'word2vec_q1_mean', 'word2vec_q2_mean', 'q1_NN_count',\n",
    "#        'q2_NN_count', 'NN_diff', 'q1_RB_count', 'q2_RB_count', 'RB_diff',\n",
    "#        'q1_VB_count', 'q2_VB_count', 'VB_diff', 'q1_DT_count', 'q2_DT_count',\n",
    "#        'DT_diff', 'q1_JJ_count', 'q2_JJ_count', 'JJ_diff', 'q1_FW_count',\n",
    "#        'q2_FW_count', 'FW_diff', 'q1_RP_count', 'q2_RP_count', 'RP_diff',\n",
    "#        'q1_SYM_count', 'q2_SYM_count', 'SYM_diff',\n",
    "#        'document_pos_similarity_10_feature',\n",
    "#        'document_pos_similarity_3_feature', 'entities_similarity_feature',\n",
    "#        'heads_similarity_feature', 'interrogative_match_feature',\n",
    "#        'non_alphanumeric_similarity_feature',\n",
    "#        'number_of_children_similarity_5_feature', 'numbers_similarity_feature',\n",
    "#        'objects_similarity_feature', 'roots_similarity_feature',\n",
    "#        'spacy_similarity_feature', 'subject_verb_inversion_similarity_feature',\n",
    "#        'subjects_similarity_feature',\n",
    "#        'unigram_idf_cutoff_similarity_10_feature',\n",
    "#        'unigram_idf_cutoff_similarity_15_feature',\n",
    "#        'unigram_idf_cutoff_similarity_5_feature',\n",
    "#        'unigram_idf_mean_difference_feature']\n",
    "\n",
    "# features = ['word_share', 'start_with_same_world', 'q1_char_num', 'q2_char_num',\n",
    "#        'q1_word_num', 'q2_word_num', 'rfidf_share', 'char_difference',\n",
    "#        'word_difference', 'seq_simhash_distance', 'shingle_simhash_distance',\n",
    "#        'avg_word_len_q1', 'avg_word_len_q2', 'avg_word_difference',\n",
    "#        'unigrams_common_count', 'bigrams_common_count',\n",
    "#        'unigrams_common_ratio', 'bigrams_common_ratio', 'cosin_sim',\n",
    "#        'word2vec_q1_mean', 'word2vec_q2_mean', 'q1_NN_count', 'q2_NN_count',\n",
    "#        'NN_diff', 'q1_RB_count', 'q2_RB_count', 'RB_diff', 'q1_VB_count',\n",
    "#        'q2_VB_count', 'VB_diff', 'q1_DT_count', 'q2_DT_count', 'DT_diff',\n",
    "#        'q1_JJ_count', 'q2_JJ_count', 'JJ_diff', 'q1_FW_count', 'q2_FW_count',\n",
    "#        'FW_diff', 'q1_RP_count', 'q2_RP_count', 'RP_diff', 'q1_SYM_count',\n",
    "#        'q2_SYM_count', 'SYM_diff']\n",
    "\n",
    "# features = ['cosin_sim', 'word_share', 'q1_char_num', 'q1_word_num', 'q2_char_num', 'q2_word_num',\n",
    "#             'start_with_same_world', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "#             'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1', 'avg_word_len_q2',\n",
    "#             'avg_word_difference', 'unigrams_common_count', 'bigrams_common_count', 'unigrams_common_ratio',\n",
    "#             'bigrams_common_ratio', 'word2vec_q1_mean', 'word2vec_q2_mean']\n",
    "\n",
    "target = 'is_duplicate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 95596 unique words in training set\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts( list(train.question1.values.astype(str)) + list(train.question2.values.astype(str)))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %d unique words in training set' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x1 = tokenizer.texts_to_sequences(train.question1.values.astype(str))\n",
    "x1 = sequence.pad_sequences(x1, maxlen=SEQ_MAX_LEN)\n",
    "\n",
    "x2 = tokenizer.texts_to_sequences(train.question2.values.astype(str))\n",
    "x2 = sequence.pad_sequences(x2, maxlen=SEQ_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = train[features]\n",
    "y = train[target]\n",
    "\n",
    "X = np.array(X)\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 30) (404290, 103) (404290,)\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape, X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402268, 103) (402268, 30) (402268,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, x1_train, x1_val, x2_train, x2_val, y_train, y_val = train_test_split(\n",
    "    X, x1, x2, y, test_size=0.005, random_state=SEED)\n",
    "\n",
    "print(X_train.shape, x1_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pretrained embeddings\n",
    "Glove pretrained word2vec, source: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Download: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "Use 300 dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 pretrained word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('../pretrained/glove.6B/glove.6B.' + str(EMBEDDING_DIM) + 'd.txt', encoding='utf-8') as embedding_file:\n",
    "    for line in embedding_file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print('Found %s pretrained word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create a embedding matrix, each row coresponds to a token (id for a word) and contains a word2vec for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95597, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Oversampling\n",
    "Oversampling leads to local validation score not matching the score from public LB on kaggle. Models with oversampling usually perform a bit better, but due to scores not maching if possible better not use it.\n",
    "\n",
    "The idea for oversampling came from Kaggle (https://www.kaggle.com/davidthaler/quora-question-pairs/how-many-1-s-are-in-the-public-lb) because the training and test set do not have the same distribution of dublicate questions. The train set has around 37% of duplicates while the private test set has 16.5% but the problem is that we only see the 35% of the prive test set. Final results are calculate on the remaining 65%, what if the distribution of the 35% set doe not match the other 65%, in that case oversampling while increasing the public LB score currently would yield in overfitting the score and poor results in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def oversample(X, y, rate=0.165):\n",
    "    pos_train = X[y == 1]\n",
    "    neg_train = X[y == 0]\n",
    "\n",
    "    # Now we oversample the negative class\n",
    "    # There is likely a much more elegant way to do this...\n",
    "    p = 0.165\n",
    "    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_train = pd.concat([neg_train, neg_train])\n",
    "        scale -=1\n",
    "    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "    print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "    X = pd.concat([pos_train, neg_train])\n",
    "    y = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "y_untouched = y_train\n",
    "\n",
    "x1_train = pd.DataFrame(x1_train) \n",
    "x2_train = pd.DataFrame(x2_train) \n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "X_train, y_train = oversample(X_train, y_untouched)\n",
    "x1_train, y_train = oversample(x1_train, y_untouched)\n",
    "x2_train, y_train = oversample(x2_train, y_untouched)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "x1_train = np.array(x1_train)\n",
    "x2_train = np.array(x2_train)\n",
    "\n",
    "print(len(X_train), len(x1_train), len(x2_train), len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Normalization\n",
    "Normalization helps but only if X is normalized, normalizing x1 and x2 does not allow the model to converge and pass the val_logloss of 0.42 -> bad. So far it seems that StandardScaler applied only on X does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "\n",
    "# Change NaN to 0\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_val = np.nan_to_num(X_val)\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, auc, f1_score\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Merge\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Dense, BatchNormalization, TimeDistributed, Input\n",
    "from keras.layers import MaxPooling1D, Lambda, Convolution1D, Flatten, SpatialDropout1D\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop, Adamax, Adagrad, Nadam\n",
    "from keras.activations import elu, relu, tanh, sigmoid\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:7: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:16: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:20: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "model_q1 = Sequential()\n",
    "model_q1.add(Embedding(len(word_index) + 1,\n",
    "                       EMBEDDING_DIM,\n",
    "                       weights=[embedding_matrix],\n",
    "                       input_length=SEQ_MAX_LEN,\n",
    "                       trainable=False,\n",
    "                       dropout=0.2))\n",
    "model_q1.add(GRU(256, recurrent_dropout=0.3, dropout=0.3, return_sequences=False))\n",
    "\n",
    "model_q2 = Sequential()\n",
    "model_q2.add(Embedding(len(word_index) + 1,\n",
    "                       EMBEDDING_DIM,\n",
    "                       weights=[embedding_matrix],\n",
    "                       input_length=SEQ_MAX_LEN,\n",
    "                       trainable=False,\n",
    "                       dropout=0.2))\n",
    "model_q2.add(GRU(256, recurrent_dropout=0.3, dropout=0.3, return_sequences=False))\n",
    "\n",
    "model_GRU = Sequential()\n",
    "model_GRU.add(Merge([model_q1, model_q2], mode = 'concat'))\n",
    "model_GRU.add(BatchNormalization())\n",
    "\n",
    "model_GRU.add(Dense(512))\n",
    "model_GRU.add(BatchNormalization())\n",
    "model_GRU.add(Activation(elu))\n",
    "model_GRU.add(Dropout(0.5))\n",
    "\n",
    "#model_GRU.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:28: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "model_sum1 = Sequential()\n",
    "model_sum1.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=SEQ_MAX_LEN,\n",
    "                     trainable=False))\n",
    "\n",
    "model_sum1.add(TimeDistributed(Dense(300)))\n",
    "model_sum1.add(BatchNormalization())\n",
    "model_sum1.add(Activation(relu))\n",
    "                 \n",
    "model_sum1.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,)))\n",
    "\n",
    "model_sum2 = Sequential()\n",
    "model_sum2.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=SEQ_MAX_LEN,\n",
    "                     trainable=False))\n",
    "\n",
    "model_sum2.add(TimeDistributed(Dense(300)))\n",
    "model_sum2.add(BatchNormalization())\n",
    "model_sum2.add(Activation(elu))\n",
    "\n",
    "model_sum2.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,)))\n",
    "\n",
    "model_sum = Sequential()\n",
    "model_sum.add(Merge([model_sum1, model_sum2], mode = 'concat'))\n",
    "model_sum.add(BatchNormalization())\n",
    "model_sum.add(Dropout(0.15))\n",
    "\n",
    "model_sum.add(Dense(512))\n",
    "model_sum.add(BatchNormalization())\n",
    "model_sum.add(Activation(elu))\n",
    "model_sum.add(Dropout(0.4))\n",
    "\n",
    "#model_sum.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:28: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "model_max1 = Sequential()\n",
    "model_max1.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=SEQ_MAX_LEN,\n",
    "                     trainable=False))\n",
    "\n",
    "model_max1.add(TimeDistributed(Dense(300)))\n",
    "model_max1.add(BatchNormalization())\n",
    "model_max1.add(Activation(elu))\n",
    "                 \n",
    "model_max1.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(300,)))\n",
    "\n",
    "model_max2 = Sequential()\n",
    "model_max2.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=SEQ_MAX_LEN,\n",
    "                     trainable=False))\n",
    "\n",
    "model_max2.add(TimeDistributed(Dense(300)))\n",
    "model_max2.add(BatchNormalization())\n",
    "model_max2.add(Activation(elu))\n",
    "\n",
    "model_max2.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(300,)))\n",
    "\n",
    "model_max = Sequential()\n",
    "model_max.add(Merge([model_max1, model_max2], mode = 'concat'))\n",
    "model_max.add(BatchNormalization())\n",
    "model_max.add(Dropout(0.15))\n",
    "\n",
    "model_max.add(Dense(512))\n",
    "model_max.add(BatchNormalization())\n",
    "model_max.add(Activation(elu))\n",
    "model_max.add(Dropout(0.4))\n",
    "\n",
    "#model_max.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:7: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:25: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:38: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "model_conv1 = Sequential()\n",
    "model_conv1.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=SEQ_MAX_LEN,\n",
    "                     trainable=False,\n",
    "                     dropout=0.15))\n",
    "\n",
    "model_conv1.add(Convolution1D(filters = 256, kernel_size = 3, padding = 'same'))\n",
    "model_conv1.add(BatchNormalization())\n",
    "model_conv1.add(Activation(relu))\n",
    "model_conv1.add(Dropout(0.4))\n",
    "\n",
    "model_conv1.add(Flatten())\n",
    "model_conv1.add(Dense(256))\n",
    "model_conv1.add(BatchNormalization())\n",
    "model_conv1.add(Activation(relu))\n",
    "\n",
    "model_conv2 = Sequential()\n",
    "model_conv2.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=SEQ_MAX_LEN,\n",
    "                     trainable=False,\n",
    "                     dropout=0.15))\n",
    "\n",
    "model_conv2.add(Convolution1D(filters = 256, kernel_size = 3, padding = 'same'))\n",
    "model_conv2.add(BatchNormalization())\n",
    "model_conv2.add(Activation(elu))\n",
    "model_conv2.add(Dropout(0.4))\n",
    "\n",
    "model_conv2.add(Flatten())\n",
    "model_conv2.add(Dense(256))\n",
    "model_conv2.add(BatchNormalization())\n",
    "model_conv2.add(Activation(elu))\n",
    "\n",
    "model_glove_conv = Sequential()\n",
    "model_glove_conv.add(Merge([model_conv1, model_conv2], mode = 'concat'))\n",
    "model_glove_conv.add(BatchNormalization())\n",
    "\n",
    "model_glove_conv.add(Dense(512))\n",
    "model_glove_conv.add(BatchNormalization())\n",
    "model_glove_conv.add(Activation(elu))\n",
    "model_glove_conv.add(Dropout(0.5))\n",
    "\n",
    "#model_glove_conv.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_features = Sequential()\n",
    "\n",
    "model_features.add(Dense(256, input_dim=X_train.shape[1]))\n",
    "model_features.add(BatchNormalization())\n",
    "model_features.add(Activation(elu))\n",
    "\n",
    "model_features.add(Dense(256,))\n",
    "model_features.add(BatchNormalization())\n",
    "model_features.add(Activation(elu))\n",
    "\n",
    "model_features.add(Dense(512))\n",
    "model_features.add(BatchNormalization())\n",
    "model_features.add(Activation(elu))\n",
    "\n",
    "model_features.add(Dense(512))\n",
    "model_features.add(BatchNormalization())\n",
    "model_features.add(Activation(elu))\n",
    "model_features.add(Dropout(0.5))\n",
    "                   \n",
    "#model_features.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:2: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "merged_model = Sequential()\n",
    "merged_model.add(Merge([model_GRU, model_sum, model_max, model_glove_conv, model_features], mode = 'concat'))\n",
    "merged_model.add(BatchNormalization())\n",
    "merged_model.add(Dropout(0.65))\n",
    "\n",
    "merged_model.add(Dense(1024))\n",
    "merged_model.add(BatchNormalization())\n",
    "merged_model.add(Activation(elu))\n",
    "merged_model.add(Dropout(0.5))\n",
    "\n",
    "merged_model.add(Dense(1024))\n",
    "merged_model.add(BatchNormalization())\n",
    "merged_model.add(Activation(elu))\n",
    "merged_model.add(Dropout(0.5))\n",
    "\n",
    "merged_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # TENSORBOARD LOGGER\n",
    "\n",
    "# keras_logger = keras.callbacks.TensorBoard(log_dir='../notebooks/tensor_logs/mergnet5',\n",
    "#                                             histogram_freq=1, write_graph=True, write_images=True)\n",
    "\n",
    "# #keras_logger.set_model(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# callbacks = [ModelCheckpoint('max.h5', monitor='val_loss', save_best_only=True,\n",
    "\n",
    "#                              mode='auto', save_weights_only=False),\n",
    "#              EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto'),\n",
    "#              TQDMNotebookCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    x1_train, x2_train,\n",
    "    x1_train, x2_train,\n",
    "    x1_train, x2_train,\n",
    "    x1_train, x2_train,\n",
    "    X_train\n",
    "]\n",
    "\n",
    "val_data = [\n",
    "    x1_val, x2_val,\n",
    "    x1_val, x2_val,\n",
    "    x1_val, x2_val,\n",
    "    x1_val, x2_val,\n",
    "    X_val\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43daac295cd4ca8bb76f21e93ddd065"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36fec62972e428fa285448b216c6fba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_model.fit(train_data, y_train,\n",
    "          batch_size=64 * 8,\n",
    "          epochs=1,\n",
    "          verbose=0,\n",
    "          validation_data=(val_data, y_val),\n",
    "          callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print all predictions\n",
    "\n",
    "preds_train = merged_model.predict(train_data, batch_size=64)\n",
    "preds_vald = merged_model.predict(val_data, batch_size=64)\n",
    "all_preds = np.vstack((preds_train, preds_vald))\n",
    "\n",
    "\n",
    "np.savetxt(\n",
    "    'trian_predictions_0_324.csv', np.c_[range(len(all_preds)), all_preds],\n",
    "    delimiter=',', header='train_id,probability', comments='', fmt='%d,%f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#merged_model = load_model('Merge.h5')\n",
    "#merged_model.save('NetSolo.h5')\n",
    "\n",
    "del x1_train, x2_train, X_train\n",
    "del x1_val, x2_val, X_val\n",
    "del train, train_old, train_bekavac, train_magic1, train_magic2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scores = merged_model.evaluate(val_data,\n",
    "                               y_val, verbose=0, batch_size=64 * 2)\n",
    "\n",
    "print(\"Model validation accuracy: %.2f\" % (scores[1]*100))\n",
    "print(\"Model validation loss: %.4f\" % (scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model histories (WILL BE MOVED SOMEWHERE ELSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model validation accuracy: 85.71\n",
    "Model validation loss: 0.3107\n",
    "\n",
    "merged_model = Sequential()\n",
    "merged_model.add(Merge([model_GRU, model_glove_embedding, model_glove_conv, model_features], mode = 'concat'))\n",
    "merged_model.add(BatchNormalization())\n",
    "merged_model.add(Dropout(0.35))\n",
    "\n",
    "merged_model.add(Dense(512))\n",
    "merged_model.add(BatchNormalization())\n",
    "merged_model.add(Activation(relu))\n",
    "merged_model.add(Dropout(0.5))\n",
    "\n",
    "merged_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate submission\n",
    "\n",
    "Chunker is used to lower RAM requirements, without chunking requirement goes up to about 24GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del x1_train, x2_train, X_train\n",
    "del x1_val, x2_val, X_val\n",
    "del X, train, y, x1, x2\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_old = pd.read_pickle('../features/test_new.pkl')\n",
    "test_bekavac = pd.read_csv('../features/test_features_bekavac_v2.csv')\n",
    "test_magic1 = pd.read_csv('../features/test_magic_feature_v1.csv')\n",
    "test_magic3 = pd.read_csv('../features/test_magic_feature_v3.csv')\n",
    "abhishek_test = pd.read_csv('../features/abhishek_test_features.csv')\n",
    "\n",
    "test_magic1 = test_magic1.drop('is_duplicate', 1)\n",
    "test_magic1 = test_magic1.drop('question2', 1)\n",
    "test_magic1 = test_magic1.drop('question1', 1)\n",
    "\n",
    "test_magic3 = test_magic3.drop('question2', 1)\n",
    "test_magic3 = test_magic3.drop('question1', 1)\n",
    "\n",
    "abhishek_test = abhishek_test.drop('question2', 1)\n",
    "abhishek_test = abhishek_test.drop('question1', 1)\n",
    "\n",
    "test_magic2 = pd.read_csv('../features/test_magic_feature_v2.csv')\n",
    "\n",
    "test = pd.concat([test_old, test_bekavac, test_magic1, test_magic2, test_magic3, abhishek_test], axis=1)\n",
    "\n",
    "del test_old, test_bekavac, test_magic1, test_magic2, test_magic3, abhishek_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def chunker(collection, chunk_size=160000):\n",
    "    chunk_num = math.ceil(collection.shape[0] / float(chunk_size))\n",
    "    for i in range(chunk_num):\n",
    "        yield collection[chunk_size*i : chunk_size*(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for q1, q2, test_row in zip(\n",
    "    chunker(test.question1), chunker(test.question2), chunker(test)\n",
    "):\n",
    "    print('%d / %d' % (len(preds), len(test)))\n",
    "    x1_test_row = tokenizer.texts_to_sequences(q1.values.astype(str))\n",
    "    x1_test_row = sequence.pad_sequences(x1_test_row, maxlen=SEQ_MAX_LEN)\n",
    "\n",
    "    x2_test_row = tokenizer.texts_to_sequences(q2.values.astype(str))\n",
    "    x2_test_row = sequence.pad_sequences(x2_test_row, maxlen=SEQ_MAX_LEN)\n",
    "    \n",
    "\n",
    "    X_test_row = test_row[features]\n",
    "    X_test_row = np.nan_to_num(X_test_row)\n",
    "    X_test_row = scaler_X.transform(X_test_row)\n",
    "\n",
    "    batch_preds = merged_model.predict([x1_test_row, x2_test_row, x1_test_row, x2_test_row, x1_test_row,\n",
    "                                        x2_test_row, x1_test_row, x2_test_row, X_test_row],\n",
    "                                       batch_size=64 * 7)\n",
    "\n",
    "    preds.extend(batch_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    '../submissions/submission_d5_nO_nC.csv', np.c_[range(len(preds)), preds],\n",
    "    delimiter=',', header='test_id,is_duplicate', comments='', fmt='%d,%f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = 0.174 / 0.37\n",
    "b = (1 - 0.174) / (1 - 0.37)\n",
    "\n",
    "def fix_predictions_for_test_distribution(x):\n",
    "    return a * x / (a * x + b * (1 - x))\n",
    "\n",
    "preds = list(map(fix_predictions_for_test_distribution, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    '../submissions/submission_d5_nO_yC.csv', np.c_[range(len(preds)), preds],\n",
    "    delimiter=',', header='test_id,is_duplicate', comments='', fmt='%d,%f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
