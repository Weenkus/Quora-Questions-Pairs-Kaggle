{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../meta/config.ini')\n",
    "config.sections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = config.get('Data', 'path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load features\n",
    "To load the features you first have to create them, run the notebook feature_engineering. Beware it takes about 2-3 hours to run so save your features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../features/train.pkl')\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_pickle('../features/test.pkl')\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Consts\n",
    "Always use constant SEED otherwise the experiment is not reproducable, in that case why are we doing it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "NUM_WORDS = 50000\n",
    "SEQ_MAX_LEN = 40\n",
    "EMBEDDING_DIM = 300 # 50, 100, 200 or 300\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# features = ['word_share', 'start_with_same_world', 'q1_char_num', 'q2_char_num',\n",
    "#        'q1_word_num', 'q2_word_num', 'rfidf_share', 'char_difference',\n",
    "#        'word_difference', 'seq_simhash_distance', 'shingle_simhash_distance',\n",
    "#        'avg_word_len_q1', 'avg_word_len_q2', 'avg_word_difference',\n",
    "#        'unigrams_common_count', 'bigrams_common_count',\n",
    "#        'unigrams_common_ratio', 'bigrams_common_ratio', 'cosin_sim',\n",
    "#        'word2vec_q1_mean', 'word2vec_q2_mean', 'q1_NN_count', 'q2_NN_count',\n",
    "#        'NN_diff', 'q1_RB_count', 'q2_RB_count', 'RB_diff', 'q1_VB_count',\n",
    "#        'q2_VB_count', 'VB_diff', 'q1_DT_count', 'q2_DT_count', 'DT_diff',\n",
    "#        'q1_JJ_count', 'q2_JJ_count', 'JJ_diff', 'q1_FW_count', 'q2_FW_count',\n",
    "#        'FW_diff', 'q1_RP_count', 'q2_RP_count', 'RP_diff', 'q1_SYM_count',\n",
    "#        'q2_SYM_count', 'SYM_diff']\n",
    "\n",
    "features = ['cosin_sim', 'word_share', 'q1_char_num', 'q1_word_num', 'q2_char_num', 'q2_word_num',\n",
    "            'start_with_same_world', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "            'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1', 'avg_word_len_q2',\n",
    "            'avg_word_difference', 'unigrams_common_count', 'bigrams_common_count', 'unigrams_common_ratio',\n",
    "            'bigrams_common_ratio', 'word2vec_q1_mean', 'word2vec_q2_mean']\n",
    "\n",
    "target = 'is_duplicate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = train[features]\n",
    "y = train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(list(train.question1.values.astype(str)) + list(train.question2.values.astype(str)))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %d unique words in training set' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x1 = tokenizer.texts_to_sequences(train.question1.values.astype(str))\n",
    "x1 = sequence.pad_sequences(x1, maxlen=SEQ_MAX_LEN)\n",
    "\n",
    "x2 = tokenizer.texts_to_sequences(train.question2.values.astype(str))\n",
    "x2 = sequence.pad_sequences(x2, maxlen=SEQ_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(x1.shape, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, x1_train, x1_val, x2_train, x2_val, y_train, y_val = train_test_split(\n",
    "    X, x1, x2, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "print(X_train.shape, x1_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pretrained embeddings\n",
    "Glove pretrained word2vec, source: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Download: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "Use 100 dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('../pretrained/glove.6B/glove.6B.' + str(EMBEDDING_DIM) + 'd.txt', encoding='utf-8') as embedding_file:\n",
    "    for line in embedding_file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print('Found %s pretrained word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create a embedding matrix, each row coresponds to a token (id for a word) and contains a word2vec for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Oversampling\n",
    "Oversampling leads to local validation score not matching the score from public LB on kaggle. Models with oversampling usually perform a bit better, but due to scores not maching if possible better not use it.\n",
    "\n",
    "The idea for oversampling came from Kaggle (https://www.kaggle.com/davidthaler/quora-question-pairs/how-many-1-s-are-in-the-public-lb) because the training and test set do not have the same distribution of dublicate questions. The train set has around 37% of duplicates while the private test set has 16.5% but the problem is that we only see the 35% of the prive test set. Final results are calculate on the remaining 65%, what if the distribution of the 35% set doe not match the other 65%, in that case oversampling while increasing the public LB score currently would yield in overfitting the score and poor results in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def oversample(X, y, rate=0.165):\n",
    "    pos_train = X[y == 1]\n",
    "    neg_train = X[y == 0]\n",
    "\n",
    "    # Now we oversample the negative class\n",
    "    # There is likely a much more elegant way to do this...\n",
    "    p = 0.165\n",
    "    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_train = pd.concat([neg_train, neg_train])\n",
    "        scale -=1\n",
    "    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "    print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "    X = pd.concat([pos_train, neg_train])\n",
    "    y = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "y_untouched = y_train\n",
    "\n",
    "x1_train = pd.DataFrame(x1_train) \n",
    "x2_train = pd.DataFrame(x2_train) \n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "X_train, y_train = oversample(X_train, y_untouched)\n",
    "x1_train, y_train = oversample(x1_train, y_untouched)\n",
    "x2_train, y_train = oversample(x2_train, y_untouched)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "x1_train = np.array(x1_train)\n",
    "x2_train = np.array(x2_train)\n",
    "\n",
    "print(len(X_train), len(x1_train), len(x2_train), len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Normalization\n",
    "Normalization helps but only if X is normalized, normalizing x1 and x2 does not allow the model to converge and pass the val_logloss of 0.42 -> bad. So far it seems that StandardScaler applied only on X does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# scaler_x1 = MinMaxScaler()\n",
    "# x1 = scaler_x1.fit_transform(x1)\n",
    "\n",
    "# scaler_x2 = MinMaxScaler()\n",
    "# x2 = scaler_x2.fit_transform(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, auc, f1_score\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Merge\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Dense, BatchNormalization, TimeDistributed, Input\n",
    "from keras.layers import MaxPooling1D, Lambda, Convolution1D, Flatten, SpatialDropout1D\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop, Adamax, Adagrad, Nadam\n",
    "from keras.activations import elu, relu, tanh, sigmoid\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_q1 = Sequential()\n",
    "model_q1.add(Embedding(len(word_index) + 1,\n",
    "                       EMBEDDING_DIM,\n",
    "                       weights=[embedding_matrix],\n",
    "                       input_length=SEQ_MAX_LEN,\n",
    "                       trainable=False,\n",
    "                       dropout=0.2))\n",
    "\n",
    "model_q1.add(GRU(256, recurrent_dropout=0.3, dropout=0.3, return_sequences=False))\n",
    "\n",
    "model_q2 = Sequential()\n",
    "model_q2.add(Embedding(len(word_index) + 1,\n",
    "                       EMBEDDING_DIM,\n",
    "                       weights=[embedding_matrix],\n",
    "                       input_length=SEQ_MAX_LEN,\n",
    "                       trainable=False,\n",
    "                       dropout=0.2))\n",
    "\n",
    "model_q2.add(GRU(256, recurrent_dropout=0.3, dropout=0.3, return_sequences=False))\n",
    "\n",
    "model_GRU = Sequential()\n",
    "model_GRU.add(Merge([model_q1, model_q2], mode = 'concat'))\n",
    "model_GRU.add(BatchNormalization())\n",
    "\n",
    "model_GRU.add(Dense(512))\n",
    "model_GRU.add(BatchNormalization())\n",
    "model_GRU.add(Activation(relu))\n",
    "model_GRU.add(Dropout(0.5))\n",
    "\n",
    "model_GRU.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_glove1 = Sequential()\n",
    "model_glove1.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=SEQ_MAX_LEN,\n",
    "                     trainable=False))\n",
    "\n",
    "model_glove1.add(TimeDistributed(Dense(300)))\n",
    "model_glove1.add(BatchNormalization())\n",
    "model_glove1.add(Activation(relu))\n",
    "                 \n",
    "model_glove1.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,)))\n",
    "\n",
    "model_glove2 = Sequential()\n",
    "model_glove2.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=SEQ_MAX_LEN,\n",
    "                     trainable=False))\n",
    "\n",
    "model_glove2.add(TimeDistributed(Dense(300)))\n",
    "model_glove2.add(BatchNormalization())\n",
    "model_glove2.add(Activation(relu))\n",
    "\n",
    "model_glove2.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,)))\n",
    "\n",
    "model_glove_embedding = Sequential()\n",
    "model_glove_embedding.add(Merge([model_glove1, model_glove2], mode = 'concat'))\n",
    "model_glove_embedding.add(BatchNormalization())\n",
    "model_glove_embedding.add(Dropout(0.15))\n",
    "\n",
    "model_glove_embedding.add(Dense(512))\n",
    "model_glove_embedding.add(BatchNormalization())\n",
    "model_glove_embedding.add(Activation(relu))\n",
    "model_glove_embedding.add(Dropout(0.4))\n",
    "\n",
    "model_glove_embedding.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_conv1 = Sequential()\n",
    "model_conv1.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=SEQ_MAX_LEN,\n",
    "                     trainable=False,\n",
    "                     dropout=0.15))\n",
    "\n",
    "model_conv1.add(Convolution1D(filters = 256, kernel_size = 3, padding = 'same'))\n",
    "model_conv1.add(BatchNormalization())\n",
    "model_conv1.add(Activation(relu))\n",
    "model_conv1.add(Dropout(0.4))\n",
    "\n",
    "model_conv1.add(Flatten())\n",
    "model_conv1.add(Dense(256))\n",
    "model_conv1.add(BatchNormalization())\n",
    "model_conv1.add(Activation(relu))\n",
    "\n",
    "model_conv2 = Sequential()\n",
    "model_conv2.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=SEQ_MAX_LEN,\n",
    "                     trainable=False,\n",
    "                     dropout=0.15))\n",
    "\n",
    "model_conv2.add(Convolution1D(filters = 256, kernel_size = 3, padding = 'same'))\n",
    "model_conv2.add(BatchNormalization())\n",
    "model_conv2.add(Activation(relu))\n",
    "model_conv2.add(Dropout(0.4))\n",
    "\n",
    "model_conv2.add(Flatten())\n",
    "model_conv2.add(Dense(256))\n",
    "model_conv2.add(BatchNormalization())\n",
    "model_conv2.add(Activation(relu))\n",
    "\n",
    "model_glove_conv = Sequential()\n",
    "model_glove_conv.add(Merge([model_conv1, model_conv2], mode = 'concat'))\n",
    "model_glove_conv.add(BatchNormalization())\n",
    "\n",
    "model_glove_conv.add(Dense(512))\n",
    "model_glove_conv.add(BatchNormalization())\n",
    "model_glove_conv.add(Activation(relu))\n",
    "model_glove_conv.add(Dropout(0.5))\n",
    "\n",
    "model_glove_conv.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_features = Sequential()\n",
    "\n",
    "model_features.add(Dense(256, input_dim=X_train.shape[1]))\n",
    "model_features.add(BatchNormalization())\n",
    "model_features.add(Activation(relu))\n",
    "\n",
    "model_features.add(Dense(256,))\n",
    "model_features.add(BatchNormalization())\n",
    "model_features.add(Activation(relu))\n",
    "\n",
    "model_features.add(Dense(512))\n",
    "model_features.add(BatchNormalization())\n",
    "model_features.add(Activation(relu))\n",
    "\n",
    "model_features.add(Dense(512))\n",
    "model_features.add(BatchNormalization())\n",
    "model_features.add(Activation(relu))\n",
    "model_features.add(Dropout(0.5))\n",
    "                   \n",
    "model_features.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged_model = Sequential()\n",
    "merged_model.add(Merge([model_GRU, model_glove_embedding, model_glove_conv, model_features], mode = 'concat'))\n",
    "merged_model.add(BatchNormalization())\n",
    "merged_model.add(Dropout(0.35))\n",
    "\n",
    "merged_model.add(Dense(512))\n",
    "merged_model.add(BatchNormalization())\n",
    "merged_model.add(Activation(relu))\n",
    "merged_model.add(Dropout(0.5))\n",
    "\n",
    "merged_model.add(Dense(512))\n",
    "merged_model.add(BatchNormalization())\n",
    "merged_model.add(Activation(relu))\n",
    "merged_model.add(Dropout(0.5))\n",
    "\n",
    "merged_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TENSORBOARD LOGGER\n",
    "\n",
    "# keras_logger = keras.callbacks.TensorBoard(log_dir='../notebooks/tensor_logs/mergnet5',\n",
    "#                                            histogram_freq=1, write_graph=True, write_images=True)\n",
    "\n",
    "# keras_logger.set_model(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint('Merge.h5', monitor='val_loss', save_best_only=True,\n",
    "                             mode='auto', save_weights_only=False),\n",
    "             EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto'),\n",
    "             TQDMNotebookCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = [x1_train, x2_train, x1_train, x2_train, x1_train, x2_train, X_train]\n",
    "val_data = ([x1_val, x2_val, x1_val, x2_val, x1_val, x2_val, X_val], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged_model.fit(train_data, y_train,\n",
    "          batch_size=64 * 2,\n",
    "          epochs=1000,\n",
    "          verbose=0,\n",
    "          validation_data=val_data,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#merged_model = load_model('Merge.h5')\n",
    "#merged_model.save('NetSolo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scores = merged_model.evaluate([x1_val, x2_val, x1_val, x2_val, x1_val, x2_val, X_val],\n",
    "                               y_val, verbose=0, batch_size=64 * 4)\n",
    "\n",
    "print(\"Model validation accuracy: %.2f\" % (scores[1]*100))\n",
    "print(\"Model validation loss: %.4f\" % (scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#model_features.save('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#model_features = load_model('test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model histories (WILL BE MOVED SOMEWHERE ELSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "odel validation accuracy: 85.71\n",
    "Model validation loss: 0.3107\n",
    "\n",
    "merged_model = Sequential()\n",
    "merged_model.add(Merge([model_GRU, model_glove_embedding, model_glove_conv, model_features], mode = 'concat'))\n",
    "merged_model.add(BatchNormalization())\n",
    "merged_model.add(Dropout(0.2))\n",
    "\n",
    "merged_model.add(Dense(512))\n",
    "merged_model.add(BatchNormalization())\n",
    "merged_model.add(Activation(relu))\n",
    "merged_model.add(Dropout(0.5))\n",
    "\n",
    "merged_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate submission\n",
    "\n",
    "Chunker is used to lower RAM requirements, without chunking requirement goes up to about 24GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def chunker(collection, chunk_size=130000):\n",
    "    chunk_num = math.ceil(collection.shape[0] / float(chunk_size))\n",
    "    for i in range(chunk_num):\n",
    "        yield collection[chunk_size*i : chunk_size*(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for q1, q2, test_row in zip(\n",
    "    chunker(test.question1), chunker(test.question2), chunker(test)\n",
    "):\n",
    "    print('%d / %d' % (len(preds), len(test)))\n",
    "    x1_test_row = tokenizer.texts_to_sequences(q1.values.astype(str))\n",
    "    x1_test_row = sequence.pad_sequences(x1_test_row, maxlen=SEQ_MAX_LEN)\n",
    "\n",
    "    x2_test_row = tokenizer.texts_to_sequences(q2.values.astype(str))\n",
    "    x2_test_row = sequence.pad_sequences(x2_test_row, maxlen=SEQ_MAX_LEN)\n",
    "    \n",
    "\n",
    "    X_test_row = test_row[features]\n",
    "    X_test_row = scaler_X.transform(X_test_row)\n",
    "\n",
    "    batch_preds = merged_model.predict([x1_test_row, x2_test_row, x1_test_row,\n",
    "                                        x2_test_row, x1_test_row, x2_test_row, X_test_row],\n",
    "                                       batch_size=128 * 2)\n",
    "\n",
    "    preds.extend(batch_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = 0.175 / 0.37\n",
    "b = (1 - 0.175) / (1 - 0.37)\n",
    "\n",
    "def fix_predictions_for_test_distribution(x):\n",
    "    return a * x / (a * x + b * (1 - x))\n",
    "\n",
    "preds = list(map(fix_predictions_for_test_distribution, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    '../submissions/submission_0_3100.csv', np.c_[range(len(preds)), preds],\n",
    "    delimiter=',', header='test_id,is_duplicate', comments='', fmt='%d,%f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
